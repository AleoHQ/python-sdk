{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of an MLP neural network Leo transpilation - MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP neural networks are expressive ML models. Here, we use them for the MNIST classification task, which contains images of handwritten digits. We show the inference of MLP neural networks for the MNIST dataset is possible in a zero knowledge environment.\n",
    "\n",
    "For this, we first download the dataset, and then compute feature representations of the dataset. We then train and test an MLP neural network on the feature dataset. Afterward, we iteratively prune the network (meaning we set weights and biases close to 0 to actually 0) and fine-tune it. Then, we transpile the final MLP neural network to Leo, evaluate the Leo network and create a zero knowledge proof."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function to download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "def download_and_extract_dataset(url, save_path, folder_path):\n",
    "    \"\"\"Download and extract dataset if it doesn't exist.\"\"\"\n",
    "    if not os.path.exists(save_path):\n",
    "        print(f\"Downloading {os.path.basename(save_path)}...\")\n",
    "        response = requests.get(url)\n",
    "        with open(save_path, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "\n",
    "        decompressed_file_name = os.path.splitext(os.path.basename(save_path))[0]\n",
    "        decompressed_file_path = os.path.join(folder_path, decompressed_file_name)\n",
    "\n",
    "        with gzip.open(save_path, \"rb\") as f_in:\n",
    "            with open(decompressed_file_path, \"wb\") as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "        print(f\"{decompressed_file_name} downloaded and extracted.\")\n",
    "    else:\n",
    "        print(f\"{os.path.basename(save_path)} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-images-idx3-ubyte.gz already exists.\n",
      "train-labels-idx1-ubyte.gz already exists.\n",
      "t10k-images-idx3-ubyte.gz already exists.\n",
      "t10k-labels-idx1-ubyte.gz already exists.\n"
     ]
    }
   ],
   "source": [
    "# URLs and filenames\n",
    "file_info = [\n",
    "    (\n",
    "        \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
    "        \"train-images-idx3-ubyte.gz\",\n",
    "    ),\n",
    "    (\n",
    "        \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
    "        \"train-labels-idx1-ubyte.gz\",\n",
    "    ),\n",
    "    (\n",
    "        \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
    "        \"t10k-images-idx3-ubyte.gz\",\n",
    "    ),\n",
    "    (\n",
    "        \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",\n",
    "        \"t10k-labels-idx1-ubyte.gz\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "folder_name = \"tmp/mnist\"\n",
    "folder_path = os.path.join(os.getcwd(), folder_name)\n",
    "\n",
    "os.makedirs(folder_path, exist_ok=True)  # Create folder if it doesn't exist\n",
    "\n",
    "# Download and extract each file\n",
    "for url, file_name in file_info:\n",
    "    path_to_save = os.path.join(folder_path, file_name)\n",
    "    download_and_extract_dataset(url, path_to_save, folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function to read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def read_idx3_ubyte_image_file(filename):\n",
    "    \"\"\"Read IDX3-ubyte formatted image data.\"\"\"\n",
    "    with open(filename, \"rb\") as f:\n",
    "        magic_num = int.from_bytes(f.read(4), byteorder=\"big\")\n",
    "        num_images = int.from_bytes(f.read(4), byteorder=\"big\")\n",
    "        num_rows = int.from_bytes(f.read(4), byteorder=\"big\")\n",
    "        num_cols = int.from_bytes(f.read(4), byteorder=\"big\")\n",
    "\n",
    "        if magic_num != 2051:\n",
    "            raise ValueError(f\"Invalid magic number: {magic_num}\")\n",
    "\n",
    "        images = np.zeros((num_images, num_rows, num_cols), dtype=np.uint8)\n",
    "\n",
    "        for i in range(num_images):\n",
    "            for r in range(num_rows):\n",
    "                for c in range(num_cols):\n",
    "                    pixel = int.from_bytes(f.read(1), byteorder=\"big\")\n",
    "                    images[i, r, c] = pixel\n",
    "\n",
    "    return images\n",
    "\n",
    "\n",
    "def read_idx1_ubyte_label_file(filename):\n",
    "    \"\"\"Read IDX1-ubyte formatted label data.\"\"\"\n",
    "    with open(filename, \"rb\") as f:\n",
    "        magic_num = int.from_bytes(f.read(4), byteorder=\"big\")\n",
    "        num_labels = int.from_bytes(f.read(4), byteorder=\"big\")\n",
    "\n",
    "        if magic_num != 2049:\n",
    "            raise ValueError(f\"Invalid magic number: {magic_num}\")\n",
    "\n",
    "        labels = np.zeros(num_labels, dtype=np.uint8)\n",
    "\n",
    "        for i in range(num_labels):\n",
    "            labels[i] = int.from_bytes(f.read(1), byteorder=\"big\")\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_images: (60000, 28, 28)\n",
      "Shape of train_labels: (60000,)\n",
      "Shape of test_images: (10000, 28, 28)\n",
      "Shape of test_labels: (10000,)\n"
     ]
    }
   ],
   "source": [
    "folder_path = os.path.join(\n",
    "    os.getcwd(), folder_name\n",
    ")  # Adjust this path to where you stored the files\n",
    "\n",
    "train_images = read_idx3_ubyte_image_file(\n",
    "    os.path.join(folder_path, \"train-images-idx3-ubyte\")\n",
    ")\n",
    "train_labels = read_idx1_ubyte_label_file(\n",
    "    os.path.join(folder_path, \"train-labels-idx1-ubyte\")\n",
    ")\n",
    "test_images = read_idx3_ubyte_image_file(\n",
    "    os.path.join(folder_path, \"t10k-images-idx3-ubyte\")\n",
    ")\n",
    "test_labels = read_idx1_ubyte_label_file(\n",
    "    os.path.join(folder_path, \"t10k-labels-idx1-ubyte\")\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Shape of train_images: {train_images.shape}\"\n",
    ")  # Should output \"Shape of train_images: (60000, 28, 28)\"\n",
    "print(\n",
    "    f\"Shape of train_labels: {train_labels.shape}\"\n",
    ")  # Should output \"Shape of train_labels: (60000,)\"\n",
    "print(\n",
    "    f\"Shape of test_images: {test_images.shape}\"\n",
    ")  # Should output \"Shape of test_images: (10000, 28, 28)\"\n",
    "print(\n",
    "    f\"Shape of test_labels: {test_labels.shape}\"\n",
    ")  # Should output \"Shape of test_labels: (10000,)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the datasets to PyTorch tensors, and get a validation set\n",
    "(We use PyTorch instead of sci-kit learn to train sparse neural networks with L1 regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conver to pytorch tensors\n",
    "import torch\n",
    "\n",
    "train_images_tensor_initial = torch.from_numpy(train_images).float()\n",
    "train_labels_tensor_initial = torch.from_numpy(train_labels).long()\n",
    "test_images_tensor = torch.from_numpy(test_images).float()\n",
    "test_labels_tensor = torch.from_numpy(test_labels).long()\n",
    "\n",
    "# seed the random number generator\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# shuffle the training dataset\n",
    "indices = torch.randperm(train_images_tensor_initial.shape[0])\n",
    "train_images_tensor_shuffled = train_images_tensor_initial[indices]\n",
    "train_labels_tensor_shuffled = train_labels_tensor_initial[indices]\n",
    "\n",
    "# get a 10% validation set\n",
    "validation_size = int(train_images_tensor_shuffled.shape[0] * 0.1)\n",
    "validation_images_tensor = train_images_tensor_shuffled[:validation_size]\n",
    "validation_labels_tensor = train_labels_tensor_shuffled[:validation_size]\n",
    "train_images_tensor = train_images_tensor_shuffled[validation_size:]\n",
    "train_labels_tensor = train_labels_tensor_shuffled[validation_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels tensor shape: torch.Size([54000])\n",
      "Validation labels tensor shape: torch.Size([6000])\n",
      "Test labels tensor shape: torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train labels tensor shape:\", train_labels_tensor.shape)\n",
    "print(\"Validation labels tensor shape:\", validation_labels_tensor.shape)\n",
    "print(\"Test labels tensor shape:\", test_labels_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract feature representations of the dataset\n",
    "\n",
    "(We transform the bounding box images to 12x12 images, defined by the new_size variable. There is a trade-off in circuit constraints and ML model accuracy. You can increase the image size which will lead to a higher accuracy at the cost of more circuit constraints and thus longer proving times.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_bounding_box(img):\n",
    "    \"\"\"\n",
    "    Extract the bounding box from an MNIST image.\n",
    "\n",
    "    Args:\n",
    "        img (np.ndarray): 2D numpy array representing the MNIST image.\n",
    "\n",
    "    Returns:\n",
    "        (np.ndarray): Cropped image with the bounding box.\n",
    "    \"\"\"\n",
    "\n",
    "    # convert torch image to numpy array\n",
    "    img = img.numpy()\n",
    "\n",
    "    # Find the rows and columns where the image has non-zero pixels\n",
    "    rows = np.any(img, axis=1)\n",
    "    cols = np.any(img, axis=0)\n",
    "\n",
    "    # Find the first and last row and column indices where the image has non-zero pixels\n",
    "    rmin, rmax = np.where(rows)[0][[0, -1]]\n",
    "    cmin, cmax = np.where(cols)[0][[0, -1]]\n",
    "\n",
    "    # Return the cropped image\n",
    "    return img[rmin : rmax + 1, cmin : cmax + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "\n",
    "def get_resized_datasets(\n",
    "    train_images_tensor, validation_images_tensor, test_images_tensor, new_size\n",
    "):\n",
    "    num_train = len(train_images_tensor)\n",
    "    num_test = len(test_images_tensor)\n",
    "    num_val = len(validation_images_tensor)\n",
    "\n",
    "    train_images_tensor_resized = np.zeros((num_train, new_size**2))\n",
    "    validation_images_tensor_resized = np.zeros((num_val, new_size**2))\n",
    "    test_images_tensor_resized = np.zeros((num_test, new_size**2))\n",
    "\n",
    "    for i in range(num_train):\n",
    "        cropped_image = get_bounding_box(train_images_tensor[i].reshape(28, 28))\n",
    "        cropped_image_uint8 = np.clip(cropped_image, 0, 255).astype(np.uint8)\n",
    "        resized_image = cv2.resize(\n",
    "            cropped_image_uint8, (new_size, new_size), interpolation=cv2.INTER_AREA\n",
    "        )\n",
    "        train_images_tensor_resized[i, :] = resized_image.flatten()\n",
    "\n",
    "    for i in range(num_val):\n",
    "        cropped_image = get_bounding_box(validation_images_tensor[i].reshape(28, 28))\n",
    "        cropped_image_uint8 = np.clip(cropped_image, 0, 255).astype(np.uint8)\n",
    "        resized_image = cv2.resize(\n",
    "            cropped_image_uint8, (new_size, new_size), interpolation=cv2.INTER_AREA\n",
    "        )\n",
    "        validation_images_tensor_resized[i, :] = resized_image.flatten()\n",
    "\n",
    "    for i in range(num_test):\n",
    "        cropped_image = get_bounding_box(test_images_tensor[i].reshape(28, 28))\n",
    "        cropped_image_uint8 = np.clip(cropped_image, 0, 255).astype(np.uint8)\n",
    "        resized_image = cv2.resize(\n",
    "            cropped_image_uint8, (new_size, new_size), interpolation=cv2.INTER_AREA\n",
    "        )\n",
    "        test_images_tensor_resized[i, :] = resized_image.flatten()\n",
    "\n",
    "    return (\n",
    "        train_images_tensor_resized,\n",
    "        validation_images_tensor_resized,\n",
    "        test_images_tensor_resized,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_size = 12\n",
    "train_images_resized, val_images_resized, test_images_resized = get_resized_datasets(\n",
    "    train_images_tensor, validation_images_tensor, test_images_tensor, new_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's compute the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_haar_features(image):\n",
    "    # raise value error if the image is not square\n",
    "    if image.shape[0] != image.shape[1]:\n",
    "        raise ValueError(\"The input image must be square.\")\n",
    "\n",
    "    features = []\n",
    "\n",
    "    # Sliding window\n",
    "    for i in range(0, image.shape[0], 3):  # Slide vertically with a step of 3\n",
    "        for j in range(0, image.shape[0], 3):  # Slide horizontally with a step of 3\n",
    "\n",
    "            if i + 6 > image.shape[0] or j + 6 > image.shape[0]:\n",
    "                continue\n",
    "\n",
    "            # Extract 6x6 window\n",
    "            window = image[i : i + 6, j : j + 6]\n",
    "\n",
    "            # Horizontal feature\n",
    "            horizontal_feature_value = np.sum(window[0:3, :]) - np.sum(window[3:6, :])\n",
    "\n",
    "            # Vertical feature\n",
    "            vertical_feature_value = np.sum(window[:, 0:3]) - np.sum(window[:, 3:6])\n",
    "\n",
    "            features.append(horizontal_feature_value)\n",
    "            features.append(vertical_feature_value)\n",
    "\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "def aspect_ratio(image, threshold=0.5):\n",
    "    # Threshold the image to create a binary representation\n",
    "    bin_image = image > threshold\n",
    "    # Find the bounding box\n",
    "    row_indices, col_indices = np.nonzero(bin_image)\n",
    "    max_row, min_row = np.max(row_indices), np.min(row_indices)\n",
    "    max_col, min_col = np.max(col_indices), np.min(col_indices)\n",
    "\n",
    "    # Calculate the aspect ratio of the bounding box\n",
    "    width = max_col - min_col + 1\n",
    "    height = max_row - min_row + 1\n",
    "\n",
    "    if height == 0:  # To avoid division by zero\n",
    "        return 1.0\n",
    "\n",
    "    return width / height\n",
    "\n",
    "\n",
    "from scipy.ndimage import label\n",
    "\n",
    "\n",
    "def num_regions_below_threshold(image, threshold=0.5):\n",
    "    # Threshold the image so that pixels below the threshold are set to 1\n",
    "    # and those above the threshold are set to 0.\n",
    "    bin_image = image < threshold\n",
    "\n",
    "    # Use connected components labeling\n",
    "    labeled_array, num_features = label(bin_image)\n",
    "\n",
    "    # Return the number of unique regions\n",
    "    # (subtracting 1 as one of the labels will be the background)\n",
    "    return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute datasets\n",
    "\n",
    "num_train = len(train_images_tensor)\n",
    "num_val = len(validation_images_tensor)\n",
    "num_test = len(test_images_tensor)\n",
    "\n",
    "aspect_ratio_train = np.zeros(num_train)\n",
    "aspect_ratio_val = np.zeros(num_val)\n",
    "aspect_ratio_test = np.zeros(num_test)\n",
    "\n",
    "num_white_regions_train = np.zeros(num_train)\n",
    "num_white_regions_val = np.zeros(num_val)\n",
    "num_white_regions_test = np.zeros(num_test)\n",
    "\n",
    "haar_1 = compute_haar_features(train_images_resized[0].reshape(new_size, new_size))\n",
    "len_haar_features = len(haar_1)\n",
    "\n",
    "haar_train = np.zeros((num_train, len_haar_features))\n",
    "haar_val = np.zeros((num_val, len_haar_features))\n",
    "haar_test = np.zeros((num_test, len_haar_features))\n",
    "\n",
    "for i in range(num_train):\n",
    "    aspect_ratio_train[i] = aspect_ratio(train_images_tensor[i].reshape(28, 28).numpy())\n",
    "    num_white_regions_train[i] = num_regions_below_threshold(\n",
    "        train_images_tensor[i].reshape(28, 28)\n",
    "    )\n",
    "    haar_train[i, :] = compute_haar_features(\n",
    "        train_images_resized[i, :].reshape(new_size, new_size)\n",
    "    )\n",
    "\n",
    "for i in range(num_val):\n",
    "    aspect_ratio_val[i] = aspect_ratio(\n",
    "        validation_images_tensor[i].reshape(28, 28).numpy()\n",
    "    )\n",
    "    num_white_regions_val[i] = num_regions_below_threshold(\n",
    "        validation_images_tensor[i].reshape(28, 28)\n",
    "    )\n",
    "    haar_val[i, :] = compute_haar_features(\n",
    "        val_images_resized[i, :].reshape(new_size, new_size)\n",
    "    )\n",
    "\n",
    "for i in range(num_test):\n",
    "    aspect_ratio_test[i] = aspect_ratio(test_images_tensor[i].reshape(28, 28).numpy())\n",
    "    num_white_regions_test[i] = num_regions_below_threshold(\n",
    "        test_images_tensor[i].reshape(28, 28)\n",
    "    )\n",
    "    haar_test[i, :] = compute_haar_features(\n",
    "        test_images_resized[i, :].reshape(new_size, new_size)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a look at the images, and the computed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape torch.Size([28, 28])\n",
      "Label tensor(0)\n",
      "Resized image\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape (12, 12)\n",
      "Haar features: [-1413. -1567.   789.   -89.    88.   590.   461.   929.   743.  -101.\n",
      "  -311. -1153. -1046.  1922. -1208.  -974.  1161.  1035.]\n",
      "Shape of Haar features: (18,)\n",
      "Aspect ratio: 0.7\n",
      "Number of white regions: 2.0\n"
     ]
    }
   ],
   "source": [
    "image_id = 0\n",
    "\n",
    "image = train_images_tensor[image_id].reshape(28, 28)\n",
    "\n",
    "print(\"Original image\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Image shape\", image.shape)\n",
    "print(\"Label\", train_labels_tensor[image_id])\n",
    "\n",
    "print(\"Resized image\")\n",
    "\n",
    "image_resized = train_images_resized[image_id].reshape(new_size, new_size)\n",
    "\n",
    "plt.imshow(image_resized, cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Image shape\", image_resized.shape)\n",
    "\n",
    "print(\"Haar features:\", haar_train[image_id, :])\n",
    "print(\"Shape of Haar features:\", haar_train[image_id, :].shape)\n",
    "\n",
    "print(\"Aspect ratio:\", aspect_ratio_train[image_id])\n",
    "print(\"Number of white regions:\", num_white_regions_train[image_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's merge all features into one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute datasets\n",
    "\n",
    "train_features = np.zeros((num_train, len_haar_features + 2))\n",
    "val_features = np.zeros((num_val, len_haar_features + 2))\n",
    "test_features = np.zeros((num_test, len_haar_features + 2))\n",
    "\n",
    "for i in range(num_train):\n",
    "    train_features[i, :] = np.hstack(\n",
    "        (haar_train[i, :], aspect_ratio_train[i], num_white_regions_train[i])\n",
    "    )\n",
    "\n",
    "for i in range(num_val):\n",
    "    val_features[i, :] = np.hstack(\n",
    "        (haar_val[i, :], aspect_ratio_val[i], num_white_regions_val[i])\n",
    "    )\n",
    "\n",
    "for i in range(num_test):\n",
    "    test_features[i, :] = np.hstack(\n",
    "        (haar_test[i, :], aspect_ratio_test[i], num_white_regions_test[i])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training features: (54000, 20)\n",
      "First training feature vector: [-1.413e+03 -1.567e+03  7.890e+02 -8.900e+01  8.800e+01  5.900e+02\n",
      "  4.610e+02  9.290e+02  7.430e+02 -1.010e+02 -3.110e+02 -1.153e+03\n",
      " -1.046e+03  1.922e+03 -1.208e+03 -9.740e+02  1.161e+03  1.035e+03\n",
      "  7.000e-01  2.000e+00]\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of training features:\", train_features.shape)\n",
    "print(\"First training feature vector:\", train_features[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_features_normalized = torch.tensor(scaler.fit_transform(train_features)).float()\n",
    "val_features_normalized = torch.tensor(scaler.transform(val_features)).float()\n",
    "test_features_normalized = torch.tensor(scaler.transform(test_features)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First training feature vector (normalized): tensor([-1.2143, -0.8321,  0.4906,  0.1937, -0.2191,  0.0082,  0.4311,  1.1292,\n",
      "         0.6659,  0.1091, -0.3643, -1.2731, -0.6324,  2.4261, -0.8677, -0.8842,\n",
      "         1.0577,  0.2177, -0.4527,  0.4035])\n"
     ]
    }
   ],
   "source": [
    "print(\"First training feature vector (normalized):\", train_features_normalized[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the neural network and the training and test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def evaluate_model(model):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(test_features_normalized)\n",
    "        _, predicted = torch.max(test_outputs.data, 1)\n",
    "        accuracy = accuracy_score(test_labels, predicted.numpy())\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "        return accuracy\n",
    "\n",
    "\n",
    "# Define the PyTorch neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train(model):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Training loop with L1 regularization\n",
    "    lambda_l1 = 0.0001  # L1 regularization coefficient\n",
    "\n",
    "    validation_losses = []\n",
    "    epoch = 0\n",
    "\n",
    "    model_states = []\n",
    "\n",
    "    while True:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(train_features_normalized)\n",
    "\n",
    "        loss = criterion(outputs, train_labels_tensor)\n",
    "\n",
    "        # Add L1 regularization\n",
    "        l1_reg = torch.tensor(0.0, requires_grad=True)\n",
    "        for param in model.parameters():\n",
    "            l1_reg = l1_reg + torch.norm(param, 1)\n",
    "        loss += lambda_l1 * l1_reg\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print progress\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch + 1}], Loss: {loss.item():.4f}, validation loss: {validation_losses[-1]:.4f}\"\n",
    "            )\n",
    "\n",
    "        # store model state\n",
    "        model_states.append(model.state_dict())\n",
    "\n",
    "        # Compute validation loss\n",
    "        with torch.no_grad():\n",
    "            outputs = model(val_features_normalized)\n",
    "            loss = criterion(outputs, validation_labels_tensor)\n",
    "            validation_losses.append(loss.item())\n",
    "\n",
    "        # Check for early stopping if no improvement in validation loss in last 10 epochs\n",
    "        if epoch > 10 and validation_losses[-1] > validation_losses[-10]:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "    best_model_state = model_states[np.argmin(validation_losses)]\n",
    "    model.load_state_dict(best_model_state)\n",
    "    validation_loss_of_best_model = validation_losses[np.argmin(validation_losses)]\n",
    "\n",
    "    return model, epoch, validation_loss_of_best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create and train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = train_features_normalized.shape[1]\n",
    "output_dim = len(set(train_labels))  # Assuming train_labels are class indices\n",
    "hidden_dim = (input_dim + output_dim) // 2\n",
    "\n",
    "# Instantiate the model\n",
    "model = SimpleNN(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100], Loss: 1.6674, validation loss: 1.6664\n",
      "Epoch [200], Loss: 0.9882, validation loss: 0.9826\n",
      "Epoch [300], Loss: 0.6444, validation loss: 0.6358\n",
      "Epoch [400], Loss: 0.4973, validation loss: 0.4875\n",
      "Epoch [500], Loss: 0.4247, validation loss: 0.4148\n",
      "Epoch [600], Loss: 0.3842, validation loss: 0.3740\n",
      "Epoch [700], Loss: 0.3591, validation loss: 0.3488\n",
      "Epoch [800], Loss: 0.3424, validation loss: 0.3318\n",
      "Epoch [900], Loss: 0.3305, validation loss: 0.3197\n",
      "Epoch [1000], Loss: 0.3217, validation loss: 0.3106\n",
      "Epoch [1100], Loss: 0.3146, validation loss: 0.3035\n",
      "Epoch [1200], Loss: 0.3087, validation loss: 0.2977\n",
      "Epoch [1300], Loss: 0.3038, validation loss: 0.2929\n",
      "Epoch [1400], Loss: 0.2994, validation loss: 0.2887\n",
      "Epoch [1500], Loss: 0.2955, validation loss: 0.2850\n",
      "Epoch [1600], Loss: 0.2918, validation loss: 0.2816\n",
      "Epoch [1700], Loss: 0.2883, validation loss: 0.2783\n",
      "Epoch [1800], Loss: 0.2848, validation loss: 0.2751\n",
      "Epoch [1900], Loss: 0.2814, validation loss: 0.2717\n",
      "Epoch [2000], Loss: 0.2779, validation loss: 0.2682\n",
      "Epoch [2100], Loss: 0.2745, validation loss: 0.2647\n",
      "Epoch [2200], Loss: 0.2712, validation loss: 0.2611\n",
      "Epoch [2300], Loss: 0.2678, validation loss: 0.2574\n",
      "Epoch [2400], Loss: 0.2645, validation loss: 0.2537\n",
      "Epoch [2500], Loss: 0.2613, validation loss: 0.2503\n",
      "Epoch [2600], Loss: 0.2583, validation loss: 0.2470\n",
      "Epoch [2700], Loss: 0.2553, validation loss: 0.2438\n",
      "Epoch [2800], Loss: 0.2523, validation loss: 0.2406\n",
      "Epoch [2900], Loss: 0.2496, validation loss: 0.2376\n",
      "Epoch [3000], Loss: 0.2472, validation loss: 0.2350\n",
      "Epoch [3100], Loss: 0.2450, validation loss: 0.2328\n",
      "Epoch [3200], Loss: 0.2429, validation loss: 0.2307\n",
      "Epoch [3300], Loss: 0.2411, validation loss: 0.2288\n",
      "Epoch [3400], Loss: 0.2395, validation loss: 0.2271\n",
      "Epoch [3500], Loss: 0.2381, validation loss: 0.2257\n",
      "Epoch [3600], Loss: 0.2368, validation loss: 0.2243\n",
      "Epoch [3700], Loss: 0.2355, validation loss: 0.2228\n",
      "Epoch [3800], Loss: 0.2343, validation loss: 0.2215\n",
      "Epoch [3900], Loss: 0.2332, validation loss: 0.2202\n",
      "Epoch [4000], Loss: 0.2321, validation loss: 0.2189\n",
      "Epoch [4100], Loss: 0.2312, validation loss: 0.2178\n",
      "Epoch [4200], Loss: 0.2302, validation loss: 0.2166\n",
      "Epoch [4300], Loss: 0.2293, validation loss: 0.2156\n",
      "Epoch [4400], Loss: 0.2284, validation loss: 0.2145\n",
      "Epoch [4500], Loss: 0.2276, validation loss: 0.2134\n",
      "Epoch [4600], Loss: 0.2268, validation loss: 0.2125\n",
      "Epoch [4700], Loss: 0.2261, validation loss: 0.2117\n",
      "Epoch [4800], Loss: 0.2253, validation loss: 0.2107\n",
      "Epoch [4900], Loss: 0.2247, validation loss: 0.2101\n",
      "Epoch [5000], Loss: 0.2240, validation loss: 0.2094\n",
      "Epoch [5100], Loss: 0.2233, validation loss: 0.2089\n",
      "Epoch [5200], Loss: 0.2227, validation loss: 0.2085\n",
      "Epoch [5300], Loss: 0.2221, validation loss: 0.2080\n",
      "Epoch [5400], Loss: 0.2216, validation loss: 0.2076\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model, epochs, val_loss = train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.938\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.938"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's prune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_pytorch_network(\n",
    "    model, weight_threshold=1e-1, bias_threshold=1e-1\n",
    "):  # noqa: D103\n",
    "    num_weights = 0\n",
    "    num_weights_already_zero = 0\n",
    "    num_changed_weights = 0\n",
    "    num_biases = 0\n",
    "    num_biases_already_zero = 0\n",
    "    num_changed_biases = 0\n",
    "\n",
    "    # Pruning the weights\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name:\n",
    "            flattened_weights = param.data.view(-1)\n",
    "            for j, weight in enumerate(flattened_weights):\n",
    "                if weight == 0:\n",
    "                    num_weights_already_zero += 1\n",
    "                elif abs(weight) < weight_threshold:\n",
    "                    flattened_weights[j] = 0\n",
    "                    num_changed_weights += 1\n",
    "                num_weights += 1\n",
    "            param.data = flattened_weights.view(param.data.shape)\n",
    "        elif \"bias\" in name:\n",
    "            flattened_biases = param.data.view(-1)\n",
    "            for j, bias in enumerate(flattened_biases):\n",
    "                if bias == 0:\n",
    "                    num_biases_already_zero += 1\n",
    "                elif abs(bias) < bias_threshold:\n",
    "                    flattened_biases[j] = 0\n",
    "                    num_changed_biases += 1\n",
    "                num_biases += 1\n",
    "            param.data = flattened_biases.view(param.data.shape)\n",
    "\n",
    "    # Set gradients of pruned weights to zero\n",
    "    def zero_gradients_hook(grad):\n",
    "        return grad * (grad != 0)\n",
    "\n",
    "    hooks = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name or \"bias\" in name:\n",
    "            hooks.append(param.register_hook(zero_gradients_hook))\n",
    "\n",
    "    print(f\"Number of weight parameters: {num_weights}\")  # noqa: T201\n",
    "    print(f\"Number of weights already zero: {num_weights_already_zero}\")  # noqa: T201\n",
    "    print(f\"Number of changed weight parameters: {num_changed_weights}\")  # noqa: T201\n",
    "    print(f\"Number of bias parameters: {num_biases}\")  # noqa: T201\n",
    "    print(f\"Number of biases already zero: {num_biases_already_zero}\")  # noqa: T201\n",
    "    print(f\"Number of changed bias parameters: {num_changed_biases}\")  # noqa: T201\n",
    "    print(  # noqa: T201\n",
    "        f\"Percentage of weights pruned: {num_changed_weights / num_weights * 100:.2f}%\"\n",
    "    )\n",
    "    print(  # noqa: T201\n",
    "        f\"Percentage of biases pruned: {num_changed_biases / num_biases * 100:.2f}%\"\n",
    "    )\n",
    "    print(  # noqa: T201\n",
    "        f\"Remaining number of non-zero weights: {num_weights - num_changed_weights}\"\n",
    "    )\n",
    "    print(  # noqa: T201\n",
    "        f\"Remaining number of non-zero biases: {num_biases - num_changed_biases}\"\n",
    "    )\n",
    "\n",
    "    return model, hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of weight parameters: 450\n",
      "Number of weights already zero: 0\n",
      "Number of changed weight parameters: 103\n",
      "Number of bias parameters: 25\n",
      "Number of biases already zero: 0\n",
      "Number of changed bias parameters: 4\n",
      "Percentage of weights pruned: 22.89%\n",
      "Percentage of biases pruned: 16.00%\n",
      "Remaining number of non-zero weights: 347\n",
      "Remaining number of non-zero biases: 21\n",
      "Accuracy: 0.9377\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "model_pruned = copy.deepcopy(model)\n",
    "model_pruned, hooks = prune_pytorch_network(model_pruned, 1e-1, 1e-1)\n",
    "\n",
    "_ = evaluate_model(model_pruned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's have a loop of fine-tuning, pruning, and evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Number of weight parameters: 450\n",
      "Number of weights already zero: 0\n",
      "Number of changed weight parameters: 103\n",
      "Number of bias parameters: 25\n",
      "Number of biases already zero: 0\n",
      "Number of changed bias parameters: 4\n",
      "Percentage of weights pruned: 22.89%\n",
      "Percentage of biases pruned: 16.00%\n",
      "Remaining number of non-zero weights: 347\n",
      "Remaining number of non-zero biases: 21\n",
      "Now training\n",
      "Epoch [100], Loss: 0.2208, validation loss: 0.2075\n",
      "Epoch [200], Loss: 0.2198, validation loss: 0.2066\n",
      "Epoch [300], Loss: 0.2190, validation loss: 0.2061\n",
      "Epoch [400], Loss: 0.2181, validation loss: 0.2055\n",
      "Early stopping\n",
      "Iteration 1\n",
      "Number of weight parameters: 450\n",
      "Number of weights already zero: 0\n",
      "Number of changed weight parameters: 112\n",
      "Number of bias parameters: 25\n",
      "Number of biases already zero: 0\n",
      "Number of changed bias parameters: 3\n",
      "Percentage of weights pruned: 24.89%\n",
      "Percentage of biases pruned: 12.00%\n",
      "Remaining number of non-zero weights: 338\n",
      "Remaining number of non-zero biases: 22\n",
      "Now training\n",
      "Early stopping\n",
      "Early stopping\n",
      "Accuracy: 0.9394\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "val_loss_new = val_loss\n",
    "model_to_prune = copy.deepcopy(model)\n",
    "\n",
    "while True:\n",
    "    print(f\"Iteration {i}\")\n",
    "    model_pruned = copy.deepcopy(model_to_prune)\n",
    "    model_pruned, _ = prune_pytorch_network(model_pruned, 1e-1, 1e-1)\n",
    "    val_loss_old = val_loss_new\n",
    "\n",
    "    model_to_prune = copy.deepcopy(model_pruned)\n",
    "    print(\"Now training\")\n",
    "    model_to_prune, _, val_loss_new = train(model_to_prune)\n",
    "\n",
    "    if val_loss_new > val_loss_old:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "    i += 1\n",
    "\n",
    "accuracy = evaluate_model(model_pruned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's convert this model to a scikit-learn MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kp/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "def pytorch_to_sklearn(pytorch_model):\n",
    "\n",
    "    # Extract weights and biases from PyTorch model\n",
    "    fc1_weight = pytorch_model.fc1.weight.data\n",
    "    fc1_bias = pytorch_model.fc1.bias.data\n",
    "    fc2_weight = pytorch_model.fc2.weight.data\n",
    "    fc2_bias = pytorch_model.fc2.bias.data\n",
    "\n",
    "    # Get the sizes for initialization\n",
    "    input_size = fc1_weight.shape[1]\n",
    "    hidden_size = fc1_weight.shape[0]\n",
    "    output_size = fc2_weight.shape[0]\n",
    "\n",
    "    # Initialize sklearn MLP\n",
    "    sklearn_mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=(hidden_size,), activation=\"relu\", max_iter=1\n",
    "    )\n",
    "\n",
    "    # To ensure the model doesn't change the weights during the dummy fit, we set warm_start=True\n",
    "    sklearn_mlp.warm_start = True\n",
    "\n",
    "    # Dummy fit to initialize weights (necessary step before setting the weights)\n",
    "    sklearn_mlp.fit(np.zeros((output_size, input_size)), list(range(output_size)))\n",
    "\n",
    "    # Set the weights and biases\n",
    "    sklearn_mlp.coefs_[0] = fc1_weight.t().numpy()\n",
    "    sklearn_mlp.intercepts_[0] = fc1_bias.numpy()\n",
    "    sklearn_mlp.coefs_[1] = fc2_weight.t().numpy()\n",
    "    sklearn_mlp.intercepts_[1] = fc2_bias.numpy()\n",
    "\n",
    "    return sklearn_mlp\n",
    "\n",
    "\n",
    "# Convert the example PyTorch MLP to sklearn MLP\n",
    "converted_model = pytorch_to_sklearn(model_pruned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9394\n",
      "Number of neurons per layer: [20, 15, 10]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the sklearn model\n",
    "accuracy = converted_model.score(test_features_normalized.numpy(), test_labels)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "layers_sizes = [converted_model.coefs_[0].shape[0]] + [\n",
    "    coef.shape[1] for coef in converted_model.coefs_\n",
    "]\n",
    "\n",
    "print(\"Number of neurons per layer:\", layers_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's transpile this model to Leo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>\n",
      "Constraints: 201491\n",
      "Runtime for one instance: 1.7962210178375244 seconds\n",
      "Leo accuracy: 74.0 %\n",
      "Python accuracy: 74.0 %\n"
     ]
    }
   ],
   "source": [
    "from zkml import LeoTranspiler\n",
    "\n",
    "# Transpile the deceision tree into Leo code\n",
    "print(type(converted_model))\n",
    "lt = LeoTranspiler(\n",
    "    model=converted_model, validation_data=train_features_normalized[0:600].numpy()\n",
    ")\n",
    "leo_project_path = os.path.join(os.getcwd(), \"/tmp/mnist\")\n",
    "leo_project_name = \"sklearn_mlp_mnist_1\"\n",
    "lt.to_leo(\n",
    "    path=leo_project_path, project_name=leo_project_name, fixed_point_scaling_factor=16\n",
    ")\n",
    "\n",
    "# Compute the accuracy of the Leo program and the Python program on the test set\n",
    "num_test_samples = len(test_features)\n",
    "\n",
    "# let's limit the number of test stamples to 10 to make the computation faster\n",
    "num_test_samples = min(num_test_samples, 50)\n",
    "\n",
    "python_predictions = converted_model.predict(test_features)\n",
    "\n",
    "leo_predictions = np.zeros(num_test_samples)\n",
    "for i in range(num_test_samples):\n",
    "    lc = lt.run(input=test_features[i])\n",
    "    leo_predictions[i] = np.argmax(lc.output_decimal)\n",
    "\n",
    "print(f\"Constraints: {lc.circuit_constraints}\")\n",
    "print(f\"Runtime for one instance: {lc.runtime} seconds\")\n",
    "\n",
    "leo_accuracy = (\n",
    "    np.sum(leo_predictions[0:num_test_samples] == test_labels[0:num_test_samples])\n",
    "    / num_test_samples\n",
    ")\n",
    "python_accuracy = (\n",
    "    np.sum(python_predictions[0:num_test_samples] == test_labels[0:num_test_samples])\n",
    "    / num_test_samples\n",
    ")\n",
    "\n",
    "print(f\"Leo accuracy: {100*leo_accuracy} %\")\n",
    "print(f\"Python accuracy: {100*python_accuracy} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's generate a proof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constraints: 201491\n",
      "Runtime for one instance: 15.774071216583252 seconds\n",
      "\n",
      "Leo prediction: [-2015.02587890625, -1571.911376953125, 1426.609130859375, 1412.776123046875, -8980.85546875, -2928.00244140625, -8780.64306640625, 5337.92138671875, 1679.63037109375, 271.654296875]\n",
      "Python prediction: 7\n",
      "True label: 7\n",
      "\n",
      "Proof: proof1qqqsqqqqqqqqqqqpqqqqqqqqqqqy5xmjz74ut3c2uplnf7ych7jvf4zy8nunyq9t6pftrrgxegmrvn6s5s52k4rlq3j4qx03j0ddnpypqy46tmkdmxwggr7ycma9f6jynurpr8w0nv5n4eye3ztqpe5xuqkar236mnxdsqec4ahskelj9akrxqpr8puhzp25mjkh3uap9g25uu7helvz39an8tfaxqcclu64je8km9ht2nmyqgst4vcjl2yakwjdtzqydd078h04r67hh726u877zyujs5e0fnae8t0fqyjxrx98kxhgu7n4xykyar5e3w22npvhe2qg5dvqspru8ysug5zfk2xszaan80lxdj28cngzss8pd923d92r4avgywzkwuqk59vmsc55w6e3k9cp2f6sqdfj28ta3s4m5uf3vl6ehfuyxq4mpgrv5w0ktqs24xppfa8rywaelxsphwuummpl385ta9qexdcmq92f7gmch7w288l9ppht9sppare24w9mjqnmpshanq5txy3k4t4capgsdvcj04nlfu4zkscekwcfvqrymatqm9lyu0c3g8htzq9k5fzqsrzm24k5jnjmc6w03e5jejc9n0xv3l8c998nesshuvuz2z2gwzqs5mj3830xc42vv0hhg7wxcmyts7td65j3wgxqdmrsjq2wvvrtxuvmt5t49e5s4cyfaqrwrgfdr9yqucsdj9xaunphfs0hpxahz3jfjvt59vuv4ym2rsruf93c09vtavqd0z2sepf0c90myaxy72qawwkm0pwt3w0cmu9m0yfvunvgju65gp5zxlttvu5rfrkweccfrxwxrhew7dntdfvkj5tft3zgk89tu9pmqwdgj706zx0cd95n984f2rlu6gu74lavgh45d7aay3tgafg2wx73zyupftfgdyd4204kfgrnpetqpkxqhx5yxg7vmq3arsl3feufmlcwd4l4zmgsmv40wpetdmjqjasnhhvkey5jresq9wk26zu398pk0vysf80k9z4ep7sjtraccx8zl6s9x68zq3yasy8lxshwh7fs4mwfvzn3yku7yc8lxqnwvtkwlf574vm3739z2npvvztvj63n3g67vh4gp035hmxsgajd0alatkdvsvy5n776jky9y8nsq2fjw9x9xknmxkcsdtydhj20jv4x0qjec058dhkcxttv8zvjaaxhhawwaqe0ka4222qdqvqqqqqqqqqqq6lxex0v4m5y24wyfe2c6h6d3pnug8025k7ngt8zlafmhpz3lf407pjzvhry2r3cyp77zrhxr2ddsyqqa6u6qejf63y54epg9nv02m8lzh3ygg3lvhjftl84ntny4ysjm936edplktejvegwz0yls4d8pg5pqy3naeqxug55rh5uuk3yha9q679jhmvygrnxlkvg3rfgrapet6fs99ndwte93ymusxe4yeg39gcjdl9p70h2z4ffw2nun7kt3mseady844n9f3stxmkyqxu8t72jh0ytqqqqptq77d\n"
     ]
    }
   ],
   "source": [
    "zkp = lt.execute(input=test_features[0])\n",
    "\n",
    "print(f\"Constraints: {zkp.circuit_constraints}\")\n",
    "print(f\"Runtime for one instance: {zkp.runtime} seconds\\n\")\n",
    "\n",
    "print(f\"Leo prediction: {zkp.output_decimal}\")\n",
    "print(f\"Python prediction: {python_predictions[0]}\")\n",
    "print(f\"True label: {test_labels[0]}\\n\")\n",
    "\n",
    "print(f\"Proof: {zkp.proof}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot the image\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Plot the image\")\n",
    "plt.imshow(test_images_tensor[0].reshape(28, 28), cmap=\"gray\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
