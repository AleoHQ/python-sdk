{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-images-idx3-ubyte.gz already exists.\n",
      "train-labels-idx1-ubyte.gz already exists.\n",
      "t10k-images-idx3-ubyte.gz already exists.\n",
      "t10k-labels-idx1-ubyte.gz already exists.\n",
      "Shape of train_images: (60000, 28, 28)\n",
      "Shape of train_labels: (60000,)\n",
      "Shape of test_images: (10000, 28, 28)\n",
      "Shape of test_labels: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# flake8: noqa: E302\n",
    "\n",
    "import gzip\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "def download_and_extract_dataset(url, save_path, folder_path):\n",
    "    \"\"\"Download and extract dataset if it doesn't exist.\"\"\"\n",
    "    if not os.path.exists(save_path):\n",
    "        print(f\"Downloading {os.path.basename(save_path)}...\")\n",
    "        response = requests.get(url)\n",
    "        with open(save_path, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "\n",
    "        decompressed_file_name = os.path.splitext(os.path.basename(save_path))[0]\n",
    "        decompressed_file_path = os.path.join(folder_path, decompressed_file_name)\n",
    "\n",
    "        with gzip.open(save_path, \"rb\") as f_in:\n",
    "            with open(decompressed_file_path, \"wb\") as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "        print(f\"{decompressed_file_name} downloaded and extracted.\")\n",
    "    else:\n",
    "        print(f\"{os.path.basename(save_path)} already exists.\")\n",
    "\n",
    "\n",
    "file_info = [\n",
    "    (\n",
    "        \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
    "        \"train-images-idx3-ubyte.gz\",\n",
    "    ),\n",
    "    (\n",
    "        \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
    "        \"train-labels-idx1-ubyte.gz\",\n",
    "    ),\n",
    "    (\n",
    "        \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
    "        \"t10k-images-idx3-ubyte.gz\",\n",
    "    ),\n",
    "    (\n",
    "        \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",\n",
    "        \"t10k-labels-idx1-ubyte.gz\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "folder_name = \"tmp/mnist\"\n",
    "folder_path = os.path.join(os.getcwd(), folder_name)\n",
    "\n",
    "os.makedirs(folder_path, exist_ok=True)  # Create folder if it doesn't exist\n",
    "\n",
    "# Download and extract each file\n",
    "for url, file_name in file_info:\n",
    "    path_to_save = os.path.join(folder_path, file_name)\n",
    "    download_and_extract_dataset(url, path_to_save, folder_path)\n",
    "\n",
    "# %%\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def read_idx3_ubyte_image_file(filename):\n",
    "    \"\"\"Read IDX3-ubyte formatted image data.\"\"\"\n",
    "    with open(filename, \"rb\") as f:\n",
    "        magic_num = int.from_bytes(f.read(4), byteorder=\"big\")\n",
    "        num_images = int.from_bytes(f.read(4), byteorder=\"big\")\n",
    "        num_rows = int.from_bytes(f.read(4), byteorder=\"big\")\n",
    "        num_cols = int.from_bytes(f.read(4), byteorder=\"big\")\n",
    "\n",
    "        if magic_num != 2051:\n",
    "            raise ValueError(f\"Invalid magic number: {magic_num}\")\n",
    "\n",
    "        images = np.zeros((num_images, num_rows, num_cols), dtype=np.uint8)\n",
    "\n",
    "        for i in range(num_images):\n",
    "            for r in range(num_rows):\n",
    "                for c in range(num_cols):\n",
    "                    pixel = int.from_bytes(f.read(1), byteorder=\"big\")\n",
    "                    images[i, r, c] = pixel\n",
    "\n",
    "    return images\n",
    "\n",
    "\n",
    "def read_idx1_ubyte_label_file(filename):\n",
    "    \"\"\"Read IDX1-ubyte formatted label data.\"\"\"\n",
    "    with open(filename, \"rb\") as f:\n",
    "        magic_num = int.from_bytes(f.read(4), byteorder=\"big\")\n",
    "        num_labels = int.from_bytes(f.read(4), byteorder=\"big\")\n",
    "\n",
    "        if magic_num != 2049:\n",
    "            raise ValueError(f\"Invalid magic number: {magic_num}\")\n",
    "\n",
    "        labels = np.zeros(num_labels, dtype=np.uint8)\n",
    "\n",
    "        for i in range(num_labels):\n",
    "            labels[i] = int.from_bytes(f.read(1), byteorder=\"big\")\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "# Example usage\n",
    "folder_path = os.path.join(\n",
    "    os.getcwd(), folder_name\n",
    ")  # Adjust this path to where you stored the files\n",
    "\n",
    "train_images = read_idx3_ubyte_image_file(\n",
    "    os.path.join(folder_path, \"train-images-idx3-ubyte\")\n",
    ")\n",
    "train_labels = read_idx1_ubyte_label_file(\n",
    "    os.path.join(folder_path, \"train-labels-idx1-ubyte\")\n",
    ")\n",
    "test_images = read_idx3_ubyte_image_file(\n",
    "    os.path.join(folder_path, \"t10k-images-idx3-ubyte\")\n",
    ")\n",
    "test_labels = read_idx1_ubyte_label_file(\n",
    "    os.path.join(folder_path, \"t10k-labels-idx1-ubyte\")\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Shape of train_images: {train_images.shape}\"\n",
    ")  # Should output \"Shape of train_images: (60000, 28, 28)\"\n",
    "print(\n",
    "    f\"Shape of train_labels: {train_labels.shape}\"\n",
    ")  # Should output \"Shape of train_labels: (60000,)\"\n",
    "print(\n",
    "    f\"Shape of test_images: {test_images.shape}\"\n",
    ")  # Should output \"Shape of test_images: (10000, 28, 28)\"\n",
    "print(\n",
    "    f\"Shape of test_labels: {test_labels.shape}\"\n",
    ")  # Should output \"Shape of test_labels: (10000,)\"\n",
    "\n",
    "# %%\n",
    "# Reshape the datasets from 3D to 2D\n",
    "train_images_2d = train_images.reshape(\n",
    "    train_images.shape[0], -1\n",
    ")  # -1 infers the size from the remaining dimensions\n",
    "test_images_2d = test_images.reshape(test_images.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transofrming image to binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "[0 1]\n",
      "(10000, 784)\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "# Thresholding\n",
    "train_images_binary = np.where(train_images_2d > 127, 1, 0)\n",
    "test_images_binary = np.where(test_images_2d > 127, 1, 0)\n",
    "\n",
    "# Check the shape and unique values to confirm the conversion\n",
    "print(train_images_binary.shape)\n",
    "print(np.unique(train_images_binary))\n",
    "print(test_images_binary.shape)\n",
    "print(np.unique(test_images_binary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue as normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_bounding_box(img):\n",
    "    \"\"\"\n",
    "    Extract the bounding box from an MNIST image.\n",
    "\n",
    "    Args:\n",
    "        img (np.ndarray): 2D numpy array representing the MNIST image.\n",
    "\n",
    "    Returns:\n",
    "        (np.ndarray): Cropped image with the bounding box.\n",
    "    \"\"\"\n",
    "\n",
    "    # Find the rows and columns where the image has non-zero pixels\n",
    "    rows = np.any(img, axis=1)\n",
    "    cols = np.any(img, axis=0)\n",
    "\n",
    "    # Find the first and last row and column indices where the image has non-zero pixels\n",
    "    rmin, rmax = np.where(rows)[0][[0, -1]]\n",
    "    cmin, cmax = np.where(cols)[0][[0, -1]]\n",
    "\n",
    "    # Return the cropped image\n",
    "    return img[rmin : rmax + 1, cmin : cmax + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape (20, 16)\n",
      "Label 0\n"
     ]
    }
   ],
   "source": [
    "image_id = 1\n",
    "\n",
    "image = train_images_binary[image_id].reshape(28, 28)\n",
    "cropped_image = get_bounding_box(image)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(cropped_image, cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Image shape\", cropped_image.shape)\n",
    "\n",
    "print(\"Label\", train_labels[image_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "cropped_image_uint8 = np.clip(cropped_image, 0, 255).astype(np.uint8)\n",
    "resized_image = cv2.resize(cropped_image_uint8, (20, 20), interpolation=cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape (20, 20)\n",
      "Label 0\n"
     ]
    }
   ],
   "source": [
    "image_id = 1\n",
    "\n",
    "plt.imshow(resized_image, cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Image shape\", resized_image.shape)\n",
    "\n",
    "print(\"Label\", train_labels[image_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and use dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = len(train_images_binary)\n",
    "\n",
    "train_features = np.zeros((num_train, 400))\n",
    "\n",
    "for i in range(num_train):\n",
    "    cropped_image = get_bounding_box(train_images_binary[i].reshape(28, 28))\n",
    "    cropped_image_uint8 = np.clip(cropped_image, 0, 255).astype(np.uint8)\n",
    "    resized_image = cv2.resize(\n",
    "        cropped_image_uint8, (20, 20), interpolation=cv2.INTER_AREA\n",
    "    )\n",
    "    train_features[i, :] = resized_image.flatten()\n",
    "\n",
    "num_test = len(test_images_binary)\n",
    "\n",
    "test_features = np.zeros((num_test, 400))\n",
    "\n",
    "for i in range(num_test):\n",
    "    cropped_image = get_bounding_box(test_images_binary[i].reshape(28, 28))\n",
    "    cropped_image_uint8 = np.clip(cropped_image, 0, 255).astype(np.uint8)\n",
    "    resized_image = cv2.resize(\n",
    "        cropped_image_uint8, (20, 20), interpolation=cv2.INTER_AREA\n",
    "    )\n",
    "    test_features[i, :] = resized_image.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(max_depth=10, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=10, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(max_depth=10, random_state=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create and train a decision tree classifier\n",
    "clf = DecisionTreeClassifier(max_depth=10, random_state=0)\n",
    "clf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python accuracy: 83.7 %\n"
     ]
    }
   ],
   "source": [
    "num_test_samples = len(test_features)\n",
    "python_predictions = clf.predict(test_features)\n",
    "python_accuracy = np.sum(python_predictions == test_labels) / num_test_samples\n",
    "print(f\"Python accuracy: {100*python_accuracy} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out an SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_features_normalized = scaler.fit_transform(train_features)\n",
    "test_features_normalized = scaler.transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC(kernel=\"rbf\", random_state=0, C=1000)\n",
    "clf.fit(train_features_normalized, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_samples = len(test_features_normalized)\n",
    "python_predictions = clf.predict(test_features_normalized)\n",
    "python_accuracy = np.sum(python_predictions == test_labels) / num_test_samples\n",
    "print(f\"Python accuracy: {100*python_accuracy} %\")\n",
    "\n",
    "print(\"Number of support vectors\", clf.n_support_)\n",
    "print(\"Total number of support vectors\", sum(clf.n_support_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out an MLP neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and initialize the MLP classifier\n",
    "mlp_large = MLPClassifier(\n",
    "    hidden_layer_sizes=(205,), max_iter=30, alpha=0.0001, solver=\"adam\", random_state=0\n",
    ")\n",
    "mlp_large.fit(train_features_normalized, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the classifier\n",
    "accuracy = mlp_large.score(test_features_normalized, test_labels)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "layers_sizes = [mlp_large.coefs_[0].shape[0]] + [\n",
    "    coef.shape[1] for coef in mlp_large.coefs_\n",
    "]\n",
    "\n",
    "print(\"Number of neurons per layer:\", layers_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out a smaller network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kp/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(24,), max_iter=30, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(24,), max_iter=30, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(24,), max_iter=30, random_state=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define and initialize the MLP classifier\n",
    "clf = MLPClassifier(\n",
    "    hidden_layer_sizes=(24,), max_iter=30, alpha=0.0001, solver=\"adam\", random_state=0\n",
    ")\n",
    "clf.fit(train_features_normalized, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9511\n",
      "Number of neurons per layer: [400, 24, 10]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the classifier\n",
    "accuracy = clf.score(test_features_normalized, test_labels)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "layers_sizes = [clf.coefs_[0].shape[0]] + [coef.shape[1] for coef in clf.coefs_]\n",
    "\n",
    "print(\"Number of neurons per layer:\", layers_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of weight parameters: 9840\n",
      "Number of changed weight parameters: 5013\n",
      "Number of bias parameters: 34\n",
      "Number of changed bias parameters: 9\n",
      "Percentage of weights pruned: 50.95%\n",
      "Percentage of biases pruned: 26.47%\n",
      "Remaining number of non-zero weights: 4827\n",
      "Remaining number of non-zero biases: 25\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "from helper import prune_neural_network\n",
    "\n",
    "pruned_network = copy.deepcopy(clf)\n",
    "pruned_network = prune_neural_network(pruned_network, 1e-1, 1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9432\n",
      "Number of neurons per layer: [400, 24, 10]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the classifier\n",
    "accuracy = pruned_network.score(test_features_normalized, test_labels)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "layers_sizes = [pruned_network.coefs_[0].shape[0]] + [\n",
    "    coef.shape[1] for coef in pruned_network.coefs_\n",
    "]\n",
    "\n",
    "print(\"Number of neurons per layer:\", layers_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune the larger network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_network = copy.deepcopy(mlp_large)\n",
    "pruned_network = prune_neural_network(pruned_network, 1e-1, 1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the classifier\n",
    "accuracy = pruned_network.score(test_features_normalized, test_labels)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "layers_sizes = [pruned_network.coefs_[0].shape[0]] + [\n",
    "    coef.shape[1] for coef in pruned_network.coefs_\n",
    "]\n",
    "\n",
    "print(\"Number of neurons per layer:\", layers_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try to train the network with L1 regularization to make it more sparse and prune it further, using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 0.3273\n",
      "Epoch [200/1000], Loss: 0.2534\n",
      "Epoch [300/1000], Loss: 0.2136\n",
      "Epoch [400/1000], Loss: 0.1875\n",
      "Epoch [500/1000], Loss: 0.1691\n",
      "Epoch [600/1000], Loss: 0.1557\n",
      "Epoch [700/1000], Loss: 0.1453\n",
      "Epoch [800/1000], Loss: 0.1371\n",
      "Epoch [900/1000], Loss: 0.1305\n",
      "Epoch [1000/1000], Loss: 0.1250\n",
      "Accuracy: 0.9709\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Convert your dataset to PyTorch tensors\n",
    "train_features_tensor = torch.tensor(train_features_normalized, dtype=torch.float32)\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "test_features_tensor = torch.tensor(test_features_normalized, dtype=torch.float32)\n",
    "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# Define the PyTorch neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = train_features_normalized.shape[1]\n",
    "hidden_dim = 70\n",
    "output_dim = len(set(train_labels))  # Assuming train_labels are class indices\n",
    "\n",
    "# Instantiate the model\n",
    "model = SimpleNN(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop with L1 regularization\n",
    "lambda_l1 = 0.0001  # L1 regularization coefficient\n",
    "num_epochs = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(train_features_tensor)\n",
    "\n",
    "    loss = criterion(outputs, train_labels_tensor)\n",
    "\n",
    "    # Add L1 regularization\n",
    "    l1_reg = torch.tensor(0.0, requires_grad=True)\n",
    "    for param in model.parameters():\n",
    "        l1_reg = l1_reg + torch.norm(param, 1)\n",
    "    loss += lambda_l1 * l1_reg\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print progress\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(test_features_tensor)\n",
    "    _, predicted = torch.max(test_outputs.data, 1)\n",
    "    accuracy = accuracy_score(test_labels, predicted.numpy())\n",
    "    print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of weight parameters: 28700\n",
      "Number of changed weight parameters: 26405\n",
      "Number of bias parameters: 80\n",
      "Number of changed bias parameters: 61\n",
      "Percentage of weights pruned: 92.00%\n",
      "Percentage of biases pruned: 76.25%\n",
      "Remaining number of non-zero weights: 2295\n",
      "Remaining number of non-zero biases: 19\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "from helper import prune_pytorch_network\n",
    "\n",
    "pruned_network = copy.deepcopy(model)\n",
    "pruned_network = prune_pytorch_network(pruned_network, 1e-1, 1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7851\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "pruned_network.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    test_outputs = pruned_network(test_features_tensor)\n",
    "    _, predicted = torch.max(test_outputs.data, 1)\n",
    "    accuracy = accuracy_score(test_labels, predicted.numpy())\n",
    "    print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.2111\n",
      "Epoch [20/100], Loss: 0.1763\n",
      "Epoch [30/100], Loss: 0.1605\n",
      "Epoch [40/100], Loss: 0.1517\n",
      "Epoch [50/100], Loss: 0.1459\n",
      "Epoch [60/100], Loss: 0.1419\n",
      "Epoch [70/100], Loss: 0.1389\n",
      "Epoch [80/100], Loss: 0.1364\n",
      "Epoch [90/100], Loss: 0.1343\n",
      "Epoch [100/100], Loss: 0.1325\n"
     ]
    }
   ],
   "source": [
    "pruned_network.train()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(pruned_network.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop with L1 regularization\n",
    "lambda_l1 = 0.0001  # L1 regularization coefficient\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = pruned_network(train_features_tensor)\n",
    "\n",
    "    loss = criterion(outputs, train_labels_tensor)\n",
    "\n",
    "    # Add L1 regularization\n",
    "    l1_reg = torch.tensor(0.0, requires_grad=True)\n",
    "    for param in pruned_network.parameters():\n",
    "        l1_reg = l1_reg + torch.norm(param, 1)\n",
    "    loss += lambda_l1 * l1_reg\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print progress\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9694\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "pruned_network.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    test_outputs = pruned_network(test_features_tensor)\n",
    "    _, predicted = torch.max(test_outputs.data, 1)\n",
    "    accuracy = accuracy_score(test_labels, predicted.numpy())\n",
    "    print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of weight parameters: 28700\n",
      "Number of changed weight parameters: 26444\n",
      "Number of bias parameters: 80\n",
      "Number of changed bias parameters: 63\n",
      "Percentage of weights pruned: 92.14%\n",
      "Percentage of biases pruned: 78.75%\n",
      "Remaining number of non-zero weights: 2256\n",
      "Remaining number of non-zero biases: 17\n"
     ]
    }
   ],
   "source": [
    "pruned_network2 = copy.deepcopy(pruned_network)\n",
    "pruned_network2 = prune_pytorch_network(pruned_network2, 1e-1, 1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8651\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "pruned_network2.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    test_outputs = pruned_network2(test_features_tensor)\n",
    "    _, predicted = torch.max(test_outputs.data, 1)\n",
    "    accuracy = accuracy_score(test_labels, predicted.numpy())\n",
    "    print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out Haar features in smaller images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_haar_features(image):\n",
    "    if image.shape != (20, 20):\n",
    "        raise ValueError(\"Input image must be of shape 28x28.\")\n",
    "\n",
    "    features = []\n",
    "\n",
    "    # Sliding window\n",
    "    for i in range(0, 20, 3):  # Slide vertically with a step of 3\n",
    "        for j in range(0, 20, 3):  # Slide horizontally with a step of 3\n",
    "\n",
    "            if i + 6 > 20 or j + 6 > 20:\n",
    "                continue\n",
    "\n",
    "            # Extract 6x6 window\n",
    "            window = image[i : i + 6, j : j + 6]\n",
    "\n",
    "            # Horizontal feature\n",
    "            horizontal_feature_value = np.sum(window[0:3, :]) - np.sum(window[3:6, :])\n",
    "\n",
    "            # Vertical feature\n",
    "            vertical_feature_value = np.sum(window[:, 0:3]) - np.sum(window[:, 3:6])\n",
    "\n",
    "            features.append(horizontal_feature_value)\n",
    "            features.append(vertical_feature_value)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.0, -5.0, -3.0, -7.0, -1.0, 1.0, 4.0, 0.0, 8.0, 6.0, 3.0, -3.0, 7.0, -7.0, 2.0, -2.0, 1.0, 7.0, 3.0, 5.0, 0.0, 0.0, 3.0, -3.0, 9.0, -5.0, 1.0, 1.0, -8.0, 4.0, 0.0, 0.0, 0.0, 0.0, -2.0, -4.0, -3.0, -9.0, -4.0, 4.0, -3.0, -3.0, -10.0, -4.0, -12.0, -4.0, -2.0, 0.0, 9.0, 5.0]\n",
      "length of a haar feature 50\n"
     ]
    }
   ],
   "source": [
    "haar_1 = compute_haar_features(train_features[0].reshape(20, 20))\n",
    "print(haar_1)\n",
    "len_haar_features = len(haar_1)\n",
    "print(\"length of a haar feature\", len_haar_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute datasets\n",
    "\n",
    "num_train = len(train_features)\n",
    "num_test = len(test_features)\n",
    "\n",
    "haar_train = np.zeros((num_train, len_haar_features))\n",
    "haar_test = np.zeros((num_test, len_haar_features))\n",
    "\n",
    "for i in range(num_train):\n",
    "    haar_train[i] = compute_haar_features(train_features[i].reshape(20, 20))\n",
    "\n",
    "for i in range(num_test):\n",
    "    haar_test[i] = compute_haar_features(test_features[i].reshape(20, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_normalized = scaler.fit_transform(haar_train)\n",
    "test_features_normalized = scaler.transform(haar_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1500], Loss: 1.1780\n",
      "Epoch [200/1500], Loss: 0.6537\n",
      "Epoch [300/1500], Loss: 0.4858\n",
      "Epoch [400/1500], Loss: 0.4064\n",
      "Epoch [500/1500], Loss: 0.3591\n",
      "Epoch [600/1500], Loss: 0.3282\n",
      "Epoch [700/1500], Loss: 0.3071\n",
      "Epoch [800/1500], Loss: 0.2921\n",
      "Epoch [900/1500], Loss: 0.2805\n",
      "Epoch [1000/1500], Loss: 0.2712\n",
      "Epoch [1100/1500], Loss: 0.2633\n",
      "Epoch [1200/1500], Loss: 0.2564\n",
      "Epoch [1300/1500], Loss: 0.2504\n",
      "Epoch [1400/1500], Loss: 0.2450\n",
      "Epoch [1500/1500], Loss: 0.2404\n",
      "Accuracy: 0.9448\n"
     ]
    }
   ],
   "source": [
    "train_features_tensor = torch.tensor(train_features_normalized, dtype=torch.float32)\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "test_features_tensor = torch.tensor(test_features_normalized, dtype=torch.float32)\n",
    "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = train_features_normalized.shape[1]\n",
    "hidden_dim = 30\n",
    "output_dim = len(set(train_labels))  # Assuming train_labels are class indices\n",
    "\n",
    "# Instantiate the model\n",
    "model = SimpleNN(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop with L1 regularization\n",
    "lambda_l1 = 0.0001  # L1 regularization coefficient\n",
    "num_epochs = 1500\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(train_features_tensor)\n",
    "\n",
    "    loss = criterion(outputs, train_labels_tensor)\n",
    "\n",
    "    # Add L1 regularization\n",
    "    l1_reg = torch.tensor(0.0, requires_grad=True)\n",
    "    for param in model.parameters():\n",
    "        l1_reg = l1_reg + torch.norm(param, 1)\n",
    "    loss += lambda_l1 * l1_reg\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print progress\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(test_features_tensor)\n",
    "    _, predicted = torch.max(test_outputs.data, 1)\n",
    "    accuracy = accuracy_score(test_labels, predicted.numpy())\n",
    "    print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of weight parameters: 1800\n",
      "Number of changed weight parameters: 560\n",
      "Number of bias parameters: 40\n",
      "Number of changed bias parameters: 11\n",
      "Percentage of weights pruned: 31.11%\n",
      "Percentage of biases pruned: 27.50%\n",
      "Remaining number of non-zero weights: 1240\n",
      "Remaining number of non-zero biases: 29\n"
     ]
    }
   ],
   "source": [
    "pruned_network = copy.deepcopy(model)\n",
    "pruned_network = prune_pytorch_network(pruned_network, 1e-1, 1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "pruned_network.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    test_outputs = pruned_network(test_features_tensor)\n",
    "    _, predicted = torch.max(test_outputs.data, 1)\n",
    "    accuracy = accuracy_score(test_labels, predicted.numpy())\n",
    "    print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
