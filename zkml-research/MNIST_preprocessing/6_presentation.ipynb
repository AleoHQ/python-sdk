{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-images-idx3-ubyte.gz already exists.\n",
      "train-labels-idx1-ubyte.gz already exists.\n",
      "t10k-images-idx3-ubyte.gz already exists.\n",
      "t10k-labels-idx1-ubyte.gz already exists.\n",
      "Shape of train_images: (60000, 28, 28)\n",
      "Shape of train_labels: (60000,)\n",
      "Shape of test_images: (10000, 28, 28)\n",
      "Shape of test_labels: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# flake8: noqa: E302\n",
    "\n",
    "import gzip\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "def download_and_extract_dataset(url, save_path, folder_path):\n",
    "    \"\"\"Download and extract dataset if it doesn't exist.\"\"\"\n",
    "    if not os.path.exists(save_path):\n",
    "        print(f\"Downloading {os.path.basename(save_path)}...\")\n",
    "        response = requests.get(url)\n",
    "        with open(save_path, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "\n",
    "        decompressed_file_name = os.path.splitext(os.path.basename(save_path))[0]\n",
    "        decompressed_file_path = os.path.join(folder_path, decompressed_file_name)\n",
    "\n",
    "        with gzip.open(save_path, \"rb\") as f_in:\n",
    "            with open(decompressed_file_path, \"wb\") as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "        print(f\"{decompressed_file_name} downloaded and extracted.\")\n",
    "    else:\n",
    "        print(f\"{os.path.basename(save_path)} already exists.\")\n",
    "\n",
    "\n",
    "file_info = [\n",
    "    (\n",
    "        \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
    "        \"train-images-idx3-ubyte.gz\",\n",
    "    ),\n",
    "    (\n",
    "        \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
    "        \"train-labels-idx1-ubyte.gz\",\n",
    "    ),\n",
    "    (\n",
    "        \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
    "        \"t10k-images-idx3-ubyte.gz\",\n",
    "    ),\n",
    "    (\n",
    "        \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",\n",
    "        \"t10k-labels-idx1-ubyte.gz\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "folder_name = \"tmp/mnist\"\n",
    "folder_path = os.path.join(os.getcwd(), folder_name)\n",
    "\n",
    "os.makedirs(folder_path, exist_ok=True)  # Create folder if it doesn't exist\n",
    "\n",
    "# Download and extract each file\n",
    "for url, file_name in file_info:\n",
    "    path_to_save = os.path.join(folder_path, file_name)\n",
    "    download_and_extract_dataset(url, path_to_save, folder_path)\n",
    "\n",
    "# %%\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def read_idx3_ubyte_image_file(filename):\n",
    "    \"\"\"Read IDX3-ubyte formatted image data.\"\"\"\n",
    "    with open(filename, \"rb\") as f:\n",
    "        magic_num = int.from_bytes(f.read(4), byteorder=\"big\")\n",
    "        num_images = int.from_bytes(f.read(4), byteorder=\"big\")\n",
    "        num_rows = int.from_bytes(f.read(4), byteorder=\"big\")\n",
    "        num_cols = int.from_bytes(f.read(4), byteorder=\"big\")\n",
    "\n",
    "        if magic_num != 2051:\n",
    "            raise ValueError(f\"Invalid magic number: {magic_num}\")\n",
    "\n",
    "        images = np.zeros((num_images, num_rows, num_cols), dtype=np.uint8)\n",
    "\n",
    "        for i in range(num_images):\n",
    "            for r in range(num_rows):\n",
    "                for c in range(num_cols):\n",
    "                    pixel = int.from_bytes(f.read(1), byteorder=\"big\")\n",
    "                    images[i, r, c] = pixel\n",
    "\n",
    "    return images\n",
    "\n",
    "\n",
    "def read_idx1_ubyte_label_file(filename):\n",
    "    \"\"\"Read IDX1-ubyte formatted label data.\"\"\"\n",
    "    with open(filename, \"rb\") as f:\n",
    "        magic_num = int.from_bytes(f.read(4), byteorder=\"big\")\n",
    "        num_labels = int.from_bytes(f.read(4), byteorder=\"big\")\n",
    "\n",
    "        if magic_num != 2049:\n",
    "            raise ValueError(f\"Invalid magic number: {magic_num}\")\n",
    "\n",
    "        labels = np.zeros(num_labels, dtype=np.uint8)\n",
    "\n",
    "        for i in range(num_labels):\n",
    "            labels[i] = int.from_bytes(f.read(1), byteorder=\"big\")\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "# Example usage\n",
    "folder_path = os.path.join(\n",
    "    os.getcwd(), folder_name\n",
    ")  # Adjust this path to where you stored the files\n",
    "\n",
    "train_images = read_idx3_ubyte_image_file(\n",
    "    os.path.join(folder_path, \"train-images-idx3-ubyte\")\n",
    ")\n",
    "train_labels = read_idx1_ubyte_label_file(\n",
    "    os.path.join(folder_path, \"train-labels-idx1-ubyte\")\n",
    ")\n",
    "test_images = read_idx3_ubyte_image_file(\n",
    "    os.path.join(folder_path, \"t10k-images-idx3-ubyte\")\n",
    ")\n",
    "test_labels = read_idx1_ubyte_label_file(\n",
    "    os.path.join(folder_path, \"t10k-labels-idx1-ubyte\")\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Shape of train_images: {train_images.shape}\"\n",
    ")  # Should output \"Shape of train_images: (60000, 28, 28)\"\n",
    "print(\n",
    "    f\"Shape of train_labels: {train_labels.shape}\"\n",
    ")  # Should output \"Shape of train_labels: (60000,)\"\n",
    "print(\n",
    "    f\"Shape of test_images: {test_images.shape}\"\n",
    ")  # Should output \"Shape of test_images: (10000, 28, 28)\"\n",
    "print(\n",
    "    f\"Shape of test_labels: {test_labels.shape}\"\n",
    ")  # Should output \"Shape of test_labels: (10000,)\"\n",
    "\n",
    "# %%\n",
    "# Reshape the datasets from 3D to 2D\n",
    "train_images_2d = train_images.reshape(\n",
    "    train_images.shape[0], -1\n",
    ")  # -1 infers the size from the remaining dimensions\n",
    "test_images_2d = test_images.reshape(test_images.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conver to pytorch tensors\n",
    "import torch\n",
    "\n",
    "train_images_tensor_initial = torch.from_numpy(train_images_2d).float()\n",
    "train_labels_tensor_initial = torch.from_numpy(train_labels).long()\n",
    "test_images_tensor = torch.from_numpy(test_images_2d).float()\n",
    "test_labels_tensor = torch.from_numpy(test_labels).long()\n",
    "\n",
    "# seed the random number generator\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# shuffle the training dataset\n",
    "indices = torch.randperm(train_images_tensor_initial.shape[0])\n",
    "train_images_tensor_shuffled = train_images_tensor_initial[indices]\n",
    "train_labels_tensor_shuffled = train_labels_tensor_initial[indices]\n",
    "\n",
    "# get a 10% validation set\n",
    "validation_size = int(train_images_tensor_shuffled.shape[0] * 0.1)\n",
    "validation_images_tensor = train_images_tensor_shuffled[:validation_size]\n",
    "validation_labels_tensor = train_labels_tensor_shuffled[:validation_size]\n",
    "train_images_tensor = train_images_tensor_shuffled[validation_size:]\n",
    "train_labels_tensor = train_labels_tensor_shuffled[validation_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([54000])\n",
      "torch.Size([6000])\n",
      "torch.Size([60000])\n",
      "torch.Size([60000])\n",
      "6000\n"
     ]
    }
   ],
   "source": [
    "print(train_labels_tensor.shape)\n",
    "print(validation_labels_tensor.shape)\n",
    "print(train_labels_tensor_initial.shape)\n",
    "print(train_labels_tensor_shuffled.shape)\n",
    "print(validation_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training labels size: 60000\n",
      "Validation set size: 6000\n",
      "Validation labels size after split: 6000\n",
      "Training labels size after split: 54000\n"
     ]
    }
   ],
   "source": [
    "print(\"Initial training labels size:\", train_labels_tensor_initial.shape[0])\n",
    "print(\"Validation set size:\", validation_size)\n",
    "print(\"Validation labels size after split:\", validation_labels_tensor.shape[0])\n",
    "print(\"Training labels size after split:\", train_labels_tensor.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAbcUlEQVR4nO3df2xV9f3H8dctPy6g7cVa29vKD8sPxYh0GZOuQxiOhrYuBIQtoP4Bm4GBxSjMH2GbIDJTZRszbAz9Y6FzE3QmAyLJyLDYkm0Fxu8YZ0O7bi1CyyTrvVCkdPTz/YOvd15pgXO5t+/28nwkn6T3nPPuefPh0BfnntNzfc45JwAAulmKdQMAgBsTAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATfa0b+KKOjg6dOHFCqamp8vl81u0AADxyzunMmTPKyclRSkrX5zk9LoBOnDihoUOHWrcBALhOjY2NGjJkSJfre9xbcKmpqdYtAADi4Go/zxMWQOvXr9cdd9yhAQMGKD8/X/v27bumOt52A4DkcLWf5wkJoLffflvLli3TypUrdfDgQeXl5amoqEinTp1KxO4AAL2RS4AJEya40tLSyOuLFy+6nJwcV1ZWdtXaUCjkJDEYDAajl49QKHTFn/dxPwO6cOGCDhw4oMLCwsiylJQUFRYWqrq6+rLt29raFA6HowYAIPnFPYA++eQTXbx4UVlZWVHLs7Ky1NTUdNn2ZWVlCgQCkcEdcABwYzC/C2758uUKhUKR0djYaN0SAKAbxP33gDIyMtSnTx81NzdHLW9ublYwGLxse7/fL7/fH+82AAA9XNzPgPr376/x48eroqIisqyjo0MVFRUqKCiI9+4AAL1UQp6EsGzZMs2bN09f+cpXNGHCBL366qtqbW3Vd77znUTsDgDQCyUkgObMmaN///vfWrFihZqamvSlL31JO3bsuOzGBADAjcvnnHPWTXxeOBxWIBCwbgMAcJ1CoZDS0tK6XG9+FxwA4MZEAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATfa0bAHBtli5d6rlm7dq1Me2rra3Nc82AAQNi2hduXJwBAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMMHDSAEDzzzzjOea1atXe67p6OjwXHM9dYAXnAEBAEwQQAAAE3EPoBdeeEE+ny9qjBkzJt67AQD0cgm5BnTPPffovffe+99O+nKpCQAQLSHJ0LdvXwWDwUR8awBAkkjINaBjx44pJydHI0aM0KOPPqqGhoYut21ra1M4HI4aAIDkF/cAys/PV3l5uXbs2KENGzaovr5ekyZN0pkzZzrdvqysTIFAIDKGDh0a75YAAD2QzznnErmDlpYWDR8+XGvXrtVjjz122fq2tja1tbVFXofDYUIISa+7fg+oX79+nmskRf2bvFaDBg2KaV9IXqFQSGlpaV2uT/jdAYMHD9add96p2traTtf7/X75/f5EtwEA6GES/ntAZ8+eVV1dnbKzsxO9KwBALxL3AHr66adVVVWlf/7zn/rrX/+qhx56SH369NHDDz8c710BAHqxuL8Fd/z4cT388MM6ffq0brvtNt1///3as2ePbrvttnjvCgDQiyX8JgSvwuGwAoGAdRvANYvlhoJVq1Z5runOa6Xt7e2ea/Ly8jzX1NTUeK5B73G1mxB4FhwAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATPIwU+JxgMOi55siRI55rMjIyPNf0dA0NDZ5rSkpKPNd89NFHnmtgg4eRAgB6JAIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACAib7WDQCJkJ6eHlPd4sWLPdd015OtT5w44blm69atMe3r8ccf91wzbNgwzzU//OEPPdd897vf9VzT3t7uuQaJxxkQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEzyMFD2ez+fzXPPcc8/FtK+nn346pjqvPvzwQ881JSUlnmtaWlo810jS3Xff7bnmgQce8FzzyCOPeK45cuSI55qf/vSnnmuQeJwBAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMMHDSNHjpaameq7proeKxurjjz/2XHP8+PEEdNK5n/3sZ55rYnkYaSwmTJjQLftB4nEGBAAwQQABAEx4DqDdu3dr+vTpysnJkc/n09atW6PWO+e0YsUKZWdna+DAgSosLNSxY8fi1S8AIEl4DqDW1lbl5eVp/fr1na5fs2aN1q1bp9dee0179+7VTTfdpKKiIp0/f/66mwUAJA/PNyGUlJR0+cmMzjm9+uqr+tGPfqQZM2ZIkt544w1lZWVp69atmjt37vV1CwBIGnG9BlRfX6+mpiYVFhZGlgUCAeXn56u6urrTmra2NoXD4agBAEh+cQ2gpqYmSVJWVlbU8qysrMi6LyorK1MgEIiMoUOHxrMlAEAPZX4X3PLlyxUKhSKjsbHRuiUAQDeIawAFg0FJUnNzc9Ty5ubmyLov8vv9SktLixoAgOQX1wDKzc1VMBhURUVFZFk4HNbevXtVUFAQz10BAHo5z3fBnT17VrW1tZHX9fX1Onz4sNLT0zVs2DA99dRT+vGPf6zRo0crNzdXzz//vHJycjRz5sx49g0A6OU8B9D+/fujnvm0bNkySdK8efNUXl6uZ599Vq2trVq4cKFaWlp0//33a8eOHRowYED8ugYA9Ho+55yzbuLzwuGwAoGAdRtIkFj+brdt2+a5ZtKkSZ5rYlVTU+O5pri42HNNQ0OD55pY+f1+zzXnzp1LQCeX6+jo8FwT6yWA/fv3x1SHS0Kh0BWv65vfBQcAuDERQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEx4/jgG4HpkZGR4runOJ1vHYtasWZ5ruvPJ1skmJcX7/5tjqUHi8bcCADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABA8jRbd68sknrVu4ohdffNFzTV1dXQI6AZIfZ0AAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBM8DBSxGzMmDGea7797W8noJPLlZeXx1S3evVqzzUdHR0x7asnW7hwoXULuAFwBgQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEDyNFzBYvXuy5JjMz03ONc85zzZEjRzzXSMn3YNE+ffrEVJeXlxfnTuJn3759nmvq6uoS0AmuF2dAAAATBBAAwITnANq9e7emT5+unJwc+Xw+bd26NWr9/Pnz5fP5okZxcXG8+gUAJAnPAdTa2qq8vDytX7++y22Ki4t18uTJyNi8efN1NQkASD6eb0IoKSlRSUnJFbfx+/0KBoMxNwUASH4JuQZUWVmpzMxM3XXXXVq8eLFOnz7d5bZtbW0Kh8NRAwCQ/OIeQMXFxXrjjTdUUVGhV155RVVVVSopKdHFixc73b6srEyBQCAyhg4dGu+WAAA9UNx/D2ju3LmRr++9916NGzdOI0eOVGVlpaZOnXrZ9suXL9eyZcsir8PhMCEEADeAhN+GPWLECGVkZKi2trbT9X6/X2lpaVEDAJD8Eh5Ax48f1+nTp5WdnZ3oXQEAehHPb8GdPXs26mymvr5ehw8fVnp6utLT07Vq1SrNnj1bwWBQdXV1evbZZzVq1CgVFRXFtXEAQO/mOYD279+vBx54IPL6s+s38+bN04YNG3T06FH95je/UUtLi3JycjRt2jStXr1afr8/fl0DAHo9n4vlSY8JFA6HFQgErNvANaisrPRcM2nSJM81sdyaf8stt3iuSUalpaUx1a1bty7OncTP/PnzPdf89re/jX8juKpQKHTF6/o8Cw4AYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYCLuH8kNoOf42te+Zt3CFR08eNBzzfbt2xPQCSxwBgQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEDyMFeonvfe97nmu+9a1vJaCT+Pnb3/7mueY///lPAjqBBc6AAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmOBhpOjxXnnlFesW4q6wsNBzzeLFiz3X9O3bff/EP/roI881L730UgI6QW/BGRAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATPIwUPd4jjzziuebll19OQCedi+UhoWvWrPFcM2jQIM81sWpvb/dcU1RU5Lnm448/9lyD5MEZEADABAEEADDhKYDKysp03333KTU1VZmZmZo5c6Zqamqitjl//rxKS0t166236uabb9bs2bPV3Nwc16YBAL2fpwCqqqpSaWmp9uzZo507d6q9vV3Tpk1Ta2trZJulS5fq3Xff1TvvvKOqqiqdOHFCs2bNinvjAIDezdNNCDt27Ih6XV5erszMTB04cECTJ09WKBTSr3/9a23atEnf+MY3JEkbN27U3XffrT179uirX/1q/DoHAPRq13UNKBQKSZLS09MlSQcOHFB7e3vUxw2PGTNGw4YNU3V1daffo62tTeFwOGoAAJJfzAHU0dGhp556ShMnTtTYsWMlSU1NTerfv78GDx4ctW1WVpaampo6/T5lZWUKBAKRMXTo0FhbAgD0IjEHUGlpqT744AO99dZb19XA8uXLFQqFIqOxsfG6vh8AoHeI6RdRlyxZou3bt2v37t0aMmRIZHkwGNSFCxfU0tISdRbU3NysYDDY6ffy+/3y+/2xtAEA6MU8nQE557RkyRJt2bJFu3btUm5ubtT68ePHq1+/fqqoqIgsq6mpUUNDgwoKCuLTMQAgKXg6AyotLdWmTZu0bds2paamRq7rBAIBDRw4UIFAQI899piWLVum9PR0paWl6YknnlBBQQF3wAEAongKoA0bNkiSpkyZErV848aNmj9/viTp5z//uVJSUjR79my1tbWpqKhIv/rVr+LSLAAgeficc866ic8Lh8MKBALWbeAaVFZWeq6ZNGmS55r//ve/nmv+9Kc/ea6J1We/8+bFgAEDEtDJ5T788MOY6p599lnPNX/84x9j2heSVygUUlpaWpfreRYcAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMBETJ+ICkjS5s2bPddMmDDBc00sn5j74IMPeq7pTvv27fNc8/rrr3uu2b17t+caSfrHP/4RUx3gBWdAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATPAwUsQslodjNjU1ea4ZPXq055pp06Z5rpGkqVOneq556aWXPNf88pe/9Fxz6tQpzzVAT8YZEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABM+55yzbuLzwuGwAoGAdRsAgOsUCoWUlpbW5XrOgAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYMJTAJWVlem+++5TamqqMjMzNXPmTNXU1ERtM2XKFPl8vqixaNGiuDYNAOj9PAVQVVWVSktLtWfPHu3cuVPt7e2aNm2aWltbo7ZbsGCBTp48GRlr1qyJa9MAgN6vr5eNd+zYEfW6vLxcmZmZOnDggCZPnhxZPmjQIAWDwfh0CABIStd1DSgUCkmS0tPTo5a/+eabysjI0NixY7V8+XKdO3euy+/R1tamcDgcNQAANwAXo4sXL7pvfvObbuLEiVHLX3/9dbdjxw539OhR97vf/c7dfvvt7qGHHury+6xcudJJYjAYDEaSjVAodMUciTmAFi1a5IYPH+4aGxuvuF1FRYWT5Gpraztdf/78eRcKhSKjsbHRfNIYDAaDcf3jagHk6RrQZ5YsWaLt27dr9+7dGjJkyBW3zc/PlyTV1tZq5MiRl633+/3y+/2xtAEA6MU8BZBzTk888YS2bNmiyspK5ebmXrXm8OHDkqTs7OyYGgQAJCdPAVRaWqpNmzZp27ZtSk1NVVNTkyQpEAho4MCBqqur06ZNm/Tggw/q1ltv1dGjR7V06VJNnjxZ48aNS8gfAADQS3m57qMu3ufbuHGjc865hoYGN3nyZJeenu78fr8bNWqUe+aZZ676PuDnhUIh8/ctGQwGg3H942o/+33/Hyw9RjgcViAQsG4DAHCdQqGQ0tLSulzPs+AAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACZ6XAA556xbAADEwdV+nve4ADpz5ox1CwCAOLjaz3Of62GnHB0dHTpx4oRSU1Pl8/mi1oXDYQ0dOlSNjY1KS0sz6tAe83AJ83AJ83AJ83BJT5gH55zOnDmjnJwcpaR0fZ7Ttxt7uiYpKSkaMmTIFbdJS0u7oQ+wzzAPlzAPlzAPlzAPl1jPQyAQuOo2Pe4tOADAjYEAAgCY6FUB5Pf7tXLlSvn9futWTDEPlzAPlzAPlzAPl/SmeehxNyEAAG4MveoMCACQPAggAIAJAggAYIIAAgCY6DUBtH79et1xxx0aMGCA8vPztW/fPuuWut0LL7wgn88XNcaMGWPdVsLt3r1b06dPV05Ojnw+n7Zu3Rq13jmnFStWKDs7WwMHDlRhYaGOHTtm02wCXW0e5s+ff9nxUVxcbNNsgpSVlem+++5TamqqMjMzNXPmTNXU1ERtc/78eZWWlurWW2/VzTffrNmzZ6u5udmo48S4lnmYMmXKZcfDokWLjDruXK8IoLffflvLli3TypUrdfDgQeXl5amoqEinTp2ybq3b3XPPPTp58mRk/PnPf7ZuKeFaW1uVl5en9evXd7p+zZo1WrdunV577TXt3btXN910k4qKinT+/Plu7jSxrjYPklRcXBx1fGzevLkbO0y8qqoqlZaWas+ePdq5c6fa29s1bdo0tba2RrZZunSp3n33Xb3zzjuqqqrSiRMnNGvWLMOu4+9a5kGSFixYEHU8rFmzxqjjLrheYMKECa60tDTy+uLFiy4nJ8eVlZUZdtX9Vq5c6fLy8qzbMCXJbdmyJfK6o6PDBYNB95Of/CSyrKWlxfn9frd582aDDrvHF+fBOefmzZvnZsyYYdKPlVOnTjlJrqqqyjl36e++X79+7p133ols8/e//91JctXV1VZtJtwX58E5577+9a+7J5980q6pa9Djz4AuXLigAwcOqLCwMLIsJSVFhYWFqq6uNuzMxrFjx5STk6MRI0bo0UcfVUNDg3VLpurr69XU1BR1fAQCAeXn59+Qx0dlZaUyMzN11113afHixTp9+rR1SwkVCoUkSenp6ZKkAwcOqL29Pep4GDNmjIYNG5bUx8MX5+Ezb775pjIyMjR27FgtX75c586ds2ivSz3uYaRf9Mknn+jixYvKysqKWp6VlaWPPvrIqCsb+fn5Ki8v11133aWTJ09q1apVmjRpkj744AOlpqZat2eiqalJkjo9Pj5bd6MoLi7WrFmzlJubq7q6Ov3gBz9QSUmJqqur1adPH+v24q6jo0NPPfWUJk6cqLFjx0q6dDz0799fgwcPjto2mY+HzuZBkh555BENHz5cOTk5Onr0qJ577jnV1NToD3/4g2G30Xp8AOF/SkpKIl+PGzdO+fn5Gj58uH7/+9/rscceM+wMPcHcuXMjX997770aN26cRo4cqcrKSk2dOtWws8QoLS3VBx98cENcB72SruZh4cKFka/vvfdeZWdna+rUqaqrq9PIkSO7u81O9fi34DIyMtSnT5/L7mJpbm5WMBg06qpnGDx4sO68807V1tZat2Lms2OA4+NyI0aMUEZGRlIeH0uWLNH27dv1/vvvR318SzAY1IULF9TS0hK1fbIeD13NQ2fy8/MlqUcdDz0+gPr376/x48eroqIisqyjo0MVFRUqKCgw7Mze2bNnVVdXp+zsbOtWzOTm5ioYDEYdH+FwWHv37r3hj4/jx4/r9OnTSXV8OOe0ZMkSbdmyRbt27VJubm7U+vHjx6tfv35Rx0NNTY0aGhqS6ni42jx05vDhw5LUs44H67sgrsVbb73l/H6/Ky8vdx9++KFbuHChGzx4sGtqarJurVt9//vfd5WVla6+vt795S9/cYWFhS4jI8OdOnXKurWEOnPmjDt06JA7dOiQk+TWrl3rDh065P71r38555x7+eWX3eDBg922bdvc0aNH3YwZM1xubq779NNPjTuPryvNw5kzZ9zTTz/tqqurXX19vXvvvffcl7/8ZTd69Gh3/vx569bjZvHixS4QCLjKykp38uTJyDh37lxkm0WLFrlhw4a5Xbt2uf3797uCggJXUFBg2HX8XW0eamtr3Ysvvuj279/v6uvr3bZt29yIESPc5MmTjTuP1isCyDnnfvGLX7hhw4a5/v37uwkTJrg9e/ZYt9Tt5syZ47Kzs13//v3d7bff7ubMmeNqa2ut20q4999/30m6bMybN885d+lW7Oeff95lZWU5v9/vpk6d6mpqamybToArzcO5c+fctGnT3G233eb69evnhg8f7hYsWJB0/0nr7M8vyW3cuDGyzaeffuoef/xxd8stt7hBgwa5hx56yJ08edKu6QS42jw0NDS4yZMnu/T0dOf3+92oUaPcM88840KhkG3jX8DHMQAATPT4a0AAgOREAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADAxP8BUAKs16X03cwAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape torch.Size([28, 28])\n",
      "Label tensor(0)\n"
     ]
    }
   ],
   "source": [
    "image_id = 0\n",
    "\n",
    "image = train_images_tensor[image_id].reshape(28, 28)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Image shape\", image.shape)\n",
    "\n",
    "print(\"Label\", train_labels_tensor[image_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a bounding box of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_bounding_box(img):\n",
    "    \"\"\"\n",
    "    Extract the bounding box from an MNIST image.\n",
    "\n",
    "    Args:\n",
    "        img (np.ndarray): 2D numpy array representing the MNIST image.\n",
    "\n",
    "    Returns:\n",
    "        (np.ndarray): Cropped image with the bounding box.\n",
    "    \"\"\"\n",
    "\n",
    "    # convert torch image to numpy array\n",
    "    img = img.numpy()\n",
    "\n",
    "    # Find the rows and columns where the image has non-zero pixels\n",
    "    rows = np.any(img, axis=1)\n",
    "    cols = np.any(img, axis=0)\n",
    "\n",
    "    # Find the first and last row and column indices where the image has non-zero pixels\n",
    "    rmin, rmax = np.where(rows)[0][[0, -1]]\n",
    "    cmin, cmax = np.where(cols)[0][[0, -1]]\n",
    "\n",
    "    # Return the cropped image\n",
    "    return img[rmin : rmax + 1, cmin : cmax + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape (20, 14)\n",
      "Label tensor(0)\n"
     ]
    }
   ],
   "source": [
    "image = train_images_tensor[image_id].reshape(28, 28)\n",
    "cropped_image = get_bounding_box(image)\n",
    "\n",
    "plt.imshow(cropped_image, cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Image shape\", cropped_image.shape)\n",
    "\n",
    "print(\"Label\", train_labels_tensor[image_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the image quadratic (20x20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "cropped_image_uint8 = np.clip(cropped_image, 0, 255).astype(np.uint8)\n",
    "resized_image = cv2.resize(cropped_image_uint8, (20, 20), interpolation=cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape (20, 20)\n",
      "Label tensor(0)\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(resized_image, cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Image shape\", resized_image.shape)\n",
    "\n",
    "print(\"Label\", train_labels_tensor[image_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a reshaped image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = len(train_images_tensor)\n",
    "\n",
    "train_images_tensor_resized = np.zeros((num_train, 400))\n",
    "\n",
    "for i in range(num_train):\n",
    "    cropped_image = get_bounding_box(train_images_tensor[i].reshape(28, 28))\n",
    "    cropped_image_uint8 = np.clip(cropped_image, 0, 255).astype(np.uint8)\n",
    "    resized_image = cv2.resize(\n",
    "        cropped_image_uint8, (20, 20), interpolation=cv2.INTER_AREA\n",
    "    )\n",
    "    train_images_tensor_resized[i, :] = resized_image.flatten()\n",
    "\n",
    "num_test = len(test_images_tensor)\n",
    "\n",
    "num_val = len(validation_images_tensor)\n",
    "\n",
    "validation_images_tensor_resized = np.zeros((num_val, 400))\n",
    "\n",
    "for i in range(num_val):\n",
    "    cropped_image = get_bounding_box(validation_images_tensor[i].reshape(28, 28))\n",
    "    cropped_image_uint8 = np.clip(cropped_image, 0, 255).astype(np.uint8)\n",
    "    resized_image = cv2.resize(\n",
    "        cropped_image_uint8, (20, 20), interpolation=cv2.INTER_AREA\n",
    "    )\n",
    "    validation_images_tensor_resized[i, :] = resized_image.flatten()\n",
    "\n",
    "num_test = len(test_images_tensor)\n",
    "\n",
    "test_images_tensor_resized = np.zeros((num_test, 400))\n",
    "\n",
    "for i in range(num_test):\n",
    "    cropped_image = get_bounding_box(test_images_tensor[i].reshape(28, 28))\n",
    "    cropped_image_uint8 = np.clip(cropped_image, 0, 255).astype(np.uint8)\n",
    "    resized_image = cv2.resize(\n",
    "        cropped_image_uint8, (20, 20), interpolation=cv2.INTER_AREA\n",
    "    )\n",
    "    test_images_tensor_resized[i, :] = resized_image.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_features_normalized = torch.tensor(scaler.fit_transform(train_images_tensor))\n",
    "val_features_normalized = torch.tensor(scaler.transform(validation_images_tensor))\n",
    "test_features_normalized = torch.tensor(scaler.transform(test_images_tensor))\n",
    "\n",
    "train_features_resized_normalized = torch.tensor(\n",
    "    scaler.fit_transform(train_images_tensor_resized)\n",
    ")\n",
    "val_features_resized_normalized = torch.tensor(\n",
    "    scaler.transform(validation_images_tensor_resized)\n",
    ")\n",
    "test_features_resized_normalized = torch.tensor(\n",
    "    scaler.transform(test_images_tensor_resized)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_normalized = train_features_normalized.float()\n",
    "val_features_normalized = val_features_normalized.float()\n",
    "test_features_normalized = test_features_normalized.float()\n",
    "\n",
    "train_features_resized_normalized = train_features_resized_normalized.float()\n",
    "val_features_resized_normalized = val_features_resized_normalized.float()\n",
    "test_features_resized_normalized = test_features_resized_normalized.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out a medium-sized neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define code to measure accuracy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def evaluate_model(model, resized=False):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        if not resized:\n",
    "            test_outputs = model(test_features_normalized)\n",
    "        else:\n",
    "            test_outputs = model(test_features_resized_normalized)\n",
    "        _, predicted = torch.max(test_outputs.data, 1)\n",
    "        accuracy = accuracy_score(test_labels, predicted.numpy())\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100], Loss: 0.3660, validation loss: 0.1789\n",
      "Epoch [200], Loss: 0.2531, validation loss: 0.1344\n",
      "Epoch [300], Loss: 0.2079, validation loss: 0.1109\n",
      "Epoch [400], Loss: 0.1819, validation loss: 0.0982\n",
      "Epoch [500], Loss: 0.1642, validation loss: 0.0911\n",
      "Epoch [600], Loss: 0.1512, validation loss: 0.0869\n",
      "Epoch [700], Loss: 0.1409, validation loss: 0.0841\n",
      "Epoch [800], Loss: 0.1327, validation loss: 0.0821\n",
      "Epoch [900], Loss: 0.1259, validation loss: 0.0808\n",
      "Early stopping\n",
      "Accuracy: 0.979\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.979"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the PyTorch neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = train_features_normalized.shape[1]\n",
    "hidden_dim = 360\n",
    "output_dim = len(set(train_labels))  # Assuming train_labels are class indices\n",
    "\n",
    "# Instantiate the model\n",
    "model_medium = SimpleNN(\n",
    "    input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_medium.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop with L1 regularization\n",
    "lambda_l1 = 0.0001  # L1 regularization coefficient\n",
    "\n",
    "validation_losses = []\n",
    "epoch = 0\n",
    "\n",
    "model_states = []\n",
    "\n",
    "while True:\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model_medium(train_features_normalized)\n",
    "\n",
    "    loss = criterion(outputs, train_labels_tensor)\n",
    "\n",
    "    # Add L1 regularization\n",
    "    l1_reg = torch.tensor(0.0, requires_grad=True)\n",
    "    for param in model_medium.parameters():\n",
    "        l1_reg = l1_reg + torch.norm(param, 1)\n",
    "    loss += lambda_l1 * l1_reg\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print progress\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(\n",
    "            f\"Epoch [{epoch + 1}], Loss: {loss.item():.4f}, validation loss: {validation_losses[-1]:.4f}\"\n",
    "        )\n",
    "\n",
    "    # store model state\n",
    "    model_states.append(model_medium.state_dict())\n",
    "\n",
    "    # Compute validation loss\n",
    "    with torch.no_grad():\n",
    "        outputs = model_medium(val_features_normalized)\n",
    "        loss = criterion(outputs, validation_labels_tensor)\n",
    "        validation_losses.append(loss.item())\n",
    "\n",
    "    # Check for early stopping if no improvement in validation loss in last 10 epochs\n",
    "    if epoch > 10 and validation_losses[-1] > validation_losses[-10]:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "    epoch += 1\n",
    "\n",
    "best_model_state = model_states[np.argmin(validation_losses)]\n",
    "model_medium.load_state_dict(best_model_state)\n",
    "\n",
    "evaluate_model(model_medium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100], Loss: 0.4016, validation loss: 0.1481\n",
      "Epoch [200], Loss: 0.2562, validation loss: 0.0993\n",
      "Epoch [300], Loss: 0.1861, validation loss: 0.0858\n",
      "Epoch [400], Loss: 0.1451, validation loss: 0.0830\n",
      "Early stopping\n",
      "Accuracy: 0.9794\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9794"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the deeper PyTorch neural network\n",
    "class SimpleNN_deep(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim):\n",
    "        super(SimpleNN_deep, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.fc3 = nn.Linear(hidden_dim2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = train_features_normalized.shape[1]\n",
    "output_dim = len(set(train_labels))  # Assuming train_labels are class indices\n",
    "\n",
    "# Instantiate the model\n",
    "model_deep = SimpleNN_deep(\n",
    "    input_dim=input_dim, hidden_dim1=360, hidden_dim2=180, output_dim=output_dim\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_deep.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop with L1 regularization\n",
    "lambda_l1 = 0.0001  # L1 regularization coefficient\n",
    "\n",
    "validation_losses = []\n",
    "epoch = 0\n",
    "\n",
    "model_states = []\n",
    "\n",
    "while True:\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model_deep(train_features_normalized)\n",
    "\n",
    "    loss = criterion(outputs, train_labels_tensor)\n",
    "\n",
    "    # Add L1 regularization\n",
    "    l1_reg = torch.tensor(0.0, requires_grad=True)\n",
    "    for param in model_deep.parameters():\n",
    "        l1_reg = l1_reg + torch.norm(param, 1)\n",
    "    loss += lambda_l1 * l1_reg\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print progress\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(\n",
    "            f\"Epoch [{epoch + 1}], Loss: {loss.item():.4f}, validation loss: {validation_losses[-1]:.4f}\"\n",
    "        )\n",
    "\n",
    "    # store model state\n",
    "    model_states.append(model_deep.state_dict())\n",
    "\n",
    "    # Compute validation loss\n",
    "    with torch.no_grad():\n",
    "        outputs = model_deep(val_features_normalized)\n",
    "        loss = criterion(outputs, validation_labels_tensor)\n",
    "        validation_losses.append(loss.item())\n",
    "\n",
    "    # Check for early stopping if no improvement in validation loss in last 10 epochs\n",
    "    if epoch > 10 and validation_losses[-1] > validation_losses[-10]:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "    epoch += 1\n",
    "\n",
    "best_model_state = model_states[np.argmin(validation_losses)]\n",
    "model_deep.load_state_dict(best_model_state)\n",
    "\n",
    "evaluate_model(model_deep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's prune the networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of weight parameters: 285840\n",
      "Number of changed weight parameters: 280334\n",
      "Number of bias parameters: 370\n",
      "Number of changed bias parameters: 334\n",
      "Percentage of weights pruned: 98.07%\n",
      "Percentage of biases pruned: 90.27%\n",
      "Remaining number of non-zero weights: 5506\n",
      "Remaining number of non-zero biases: 36\n",
      "Accuracy: 0.9757\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9757"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "from helper import prune_pytorch_network\n",
    "\n",
    "model_medium_pruned = copy.deepcopy(model_medium)\n",
    "model_medium_pruned = prune_pytorch_network(model_medium_pruned, 5e-2, 5e-2)\n",
    "\n",
    "evaluate_model(model_medium_pruned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of weight parameters: 348840\n",
      "Number of changed weight parameters: 341472\n",
      "Number of bias parameters: 550\n",
      "Number of changed bias parameters: 507\n",
      "Percentage of weights pruned: 97.89%\n",
      "Percentage of biases pruned: 92.18%\n",
      "Remaining number of non-zero weights: 7368\n",
      "Remaining number of non-zero biases: 43\n",
      "Accuracy: 0.968\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.968"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_deep_pruned = copy.deepcopy(model_deep)\n",
    "model_deep_pruned = prune_pytorch_network(model_deep_pruned, 5e-2, 5e-2)\n",
    "\n",
    "evaluate_model(model_deep_pruned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try the resized images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100], Loss: 0.3491, validation loss: 0.1727\n",
      "Epoch [200], Loss: 0.2511, validation loss: 0.1289\n",
      "Epoch [300], Loss: 0.2029, validation loss: 0.1054\n",
      "Epoch [400], Loss: 0.1755, validation loss: 0.0912\n",
      "Epoch [500], Loss: 0.1580, validation loss: 0.0822\n",
      "Epoch [600], Loss: 0.1459, validation loss: 0.0763\n",
      "Epoch [700], Loss: 0.1367, validation loss: 0.0725\n",
      "Epoch [800], Loss: 0.1293, validation loss: 0.0698\n",
      "Epoch [900], Loss: 0.1233, validation loss: 0.0681\n",
      "Epoch [1000], Loss: 0.1182, validation loss: 0.0666\n",
      "Epoch [1100], Loss: 0.1139, validation loss: 0.0653\n",
      "Epoch [1200], Loss: 0.1103, validation loss: 0.0644\n",
      "Early stopping\n",
      "Accuracy: 0.9815\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9815"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_dim = train_features_resized_normalized.shape[1]\n",
    "hidden_dim = 205\n",
    "output_dim = len(set(train_labels))  # Assuming train_labels are class indices\n",
    "\n",
    "# Instantiate the model\n",
    "model_medium_resized_dataset = SimpleNN(\n",
    "    input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_medium_resized_dataset.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop with L1 regularization\n",
    "lambda_l1 = 0.0001  # L1 regularization coefficient\n",
    "\n",
    "validation_losses = []\n",
    "epoch = 0\n",
    "\n",
    "model_states = []\n",
    "\n",
    "while True:\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model_medium_resized_dataset(train_features_resized_normalized)\n",
    "\n",
    "    loss = criterion(outputs, train_labels_tensor)\n",
    "\n",
    "    # Add L1 regularization\n",
    "    l1_reg = torch.tensor(0.0, requires_grad=True)\n",
    "    for param in model_medium_resized_dataset.parameters():\n",
    "        l1_reg = l1_reg + torch.norm(param, 1)\n",
    "    loss += lambda_l1 * l1_reg\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print progress\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(\n",
    "            f\"Epoch [{epoch + 1}], Loss: {loss.item():.4f}, validation loss: {validation_losses[-1]:.4f}\"\n",
    "        )\n",
    "\n",
    "    # store model state\n",
    "    model_states.append(model_medium_resized_dataset.state_dict())\n",
    "\n",
    "    # Compute validation loss\n",
    "    with torch.no_grad():\n",
    "        outputs = model_medium_resized_dataset(val_features_resized_normalized)\n",
    "        loss = criterion(outputs, validation_labels_tensor)\n",
    "        validation_losses.append(loss.item())\n",
    "\n",
    "    # Check for early stopping if no improvement in validation loss in last 10 epochs\n",
    "    if epoch > 10 and validation_losses[-1] > validation_losses[-10]:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "    epoch += 1\n",
    "\n",
    "best_model_state = model_states[np.argmin(validation_losses)]\n",
    "model_medium_resized_dataset.load_state_dict(best_model_state)\n",
    "\n",
    "evaluate_model(model_medium_resized_dataset, resized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of weight parameters: 84050\n",
      "Number of changed weight parameters: 79204\n",
      "Number of bias parameters: 215\n",
      "Number of changed bias parameters: 186\n",
      "Percentage of weights pruned: 94.23%\n",
      "Percentage of biases pruned: 86.51%\n",
      "Remaining number of non-zero weights: 4846\n",
      "Remaining number of non-zero biases: 29\n",
      "Accuracy: 0.9771\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9771"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_medium_resized_dataset_pruned = copy.deepcopy(model_medium_resized_dataset)\n",
    "model_medium_resized_dataset_pruned = prune_pytorch_network(\n",
    "    model_medium_resized_dataset_pruned, 5e-2, 5e-2\n",
    ")\n",
    "\n",
    "evaluate_model(model_medium_resized_dataset_pruned, resized=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look into haar features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_haar_features(image):\n",
    "    if image.shape != (20, 20) and image.shape != (28, 28):\n",
    "        raise ValueError(\"Input image must be of shape 20x20 or 28x28.\")\n",
    "\n",
    "    features = []\n",
    "\n",
    "    # Sliding window\n",
    "    for i in range(0, image.shape[0], 3):  # Slide vertically with a step of 3\n",
    "        for j in range(0, image.shape[0], 3):  # Slide horizontally with a step of 3\n",
    "\n",
    "            if i + 6 > image.shape[0] or j + 6 > image.shape[0]:\n",
    "                continue\n",
    "\n",
    "            # Extract 6x6 window\n",
    "            window = image[i : i + 6, j : j + 6]\n",
    "\n",
    "            # Horizontal feature\n",
    "            horizontal_feature_value = np.sum(window[0:3, :]) - np.sum(window[3:6, :])\n",
    "\n",
    "            # Vertical feature\n",
    "            vertical_feature_value = np.sum(window[:, 0:3]) - np.sum(window[:, 3:6])\n",
    "\n",
    "            features.append(horizontal_feature_value)\n",
    "            features.append(vertical_feature_value)\n",
    "\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0.     0.     0.     0.     0.     0.  -117.  -117. -1480. -1246.\n",
      " -1363.  1363.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   -18.   -18. -1409. -1607. -2169. -1879. -1502.  2780.  -724.   724.\n",
      "     0.     0.     0.     0.     0.     0.  -847.  -883. -1244. -2530.\n",
      "  1435.   963.   501.  -329. -1384.  2726.   -53.    53.     0.     0.\n",
      "   -41.   -41. -1010. -2658.   508.   366.  1663.  1901.   -36. -3900.\n",
      "  -331.  4117.  -109.   215.     0.     0.  -488.  -570.  -253. -2863.\n",
      "   663.  3005.  -386.  -632.  -314. -2994.   643.  3873.   143.   181.\n",
      "     0.     0.   -47. -1105.   224. -1822.  -145.  2511. -1552. -2594.\n",
      "   -30.   562.  1125.  2429.    19.    19.     0.     0.   281.  -871.\n",
      "  -205. -2271. -1857.   939.    35.  -537.  2077.  2069.   671.   671.\n",
      "     0.     0.     0.     0.   295.  -295.  2109. -1519.  3601.    27.\n",
      "  2454.  1120.   667.   667.     0.     0.     0.     0.]\n",
      "length of a haar feature for 28x28 images 128\n"
     ]
    }
   ],
   "source": [
    "haar_1 = compute_haar_features(train_images_tensor[0].reshape(28, 28).numpy())\n",
    "print(haar_1)\n",
    "len_haar_features = len(haar_1)\n",
    "print(\"length of a haar feature for 28x28 images\", len_haar_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -339.  -339. -1844. -1810. -1511. -1207.  -131.  -249. -1376.  2298.\n",
      " -1690. -2094. -1410. -1280.  1610.  1616.  2663.  -639.   271.  -949.\n",
      " -1000. -2236.  1667.  1809.  1742.  1622.   -53. -1387.  -292. -2832.\n",
      "    98.   608.   988.  2522.  -288.  -292. -1582. -2562.  -412.  -726.\n",
      "  -110.  2028. -1076.  1026. -2496. -1492. -1082. -1242.  1760.  1966.]\n",
      "length of a haar feature for 20x20 images 50\n"
     ]
    }
   ],
   "source": [
    "haar_1 = compute_haar_features(train_images_tensor_resized[0].reshape(20, 20))\n",
    "print(haar_1)\n",
    "len_haar_features_resized = len(haar_1)\n",
    "print(\"length of a haar feature for 20x20 images\", len_haar_features_resized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further simple features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aspect_ratio(image, threshold=0.5):\n",
    "    # Threshold the image to create a binary representation\n",
    "    bin_image = image > threshold\n",
    "    # Find the bounding box\n",
    "    row_indices, col_indices = np.nonzero(bin_image)\n",
    "    max_row, min_row = np.max(row_indices), np.min(row_indices)\n",
    "    max_col, min_col = np.max(col_indices), np.min(col_indices)\n",
    "\n",
    "    # Calculate the aspect ratio of the bounding box\n",
    "    width = max_col - min_col + 1\n",
    "    height = max_row - min_row + 1\n",
    "\n",
    "    if height == 0:  # To avoid division by zero\n",
    "        return 1.0\n",
    "\n",
    "    return width / height\n",
    "\n",
    "\n",
    "from scipy.ndimage import label\n",
    "\n",
    "\n",
    "def num_regions_below_threshold(image, threshold=0.5):\n",
    "    # Threshold the image so that pixels below the threshold are set to 1\n",
    "    # and those above the threshold are set to 0.\n",
    "    bin_image = image < threshold\n",
    "\n",
    "    # Use connected components labeling\n",
    "    labeled_array, num_features = label(bin_image)\n",
    "\n",
    "    # Return the number of unique regions\n",
    "    # (subtracting 1 as one of the labels will be the background)\n",
    "    return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute datasets\n",
    "\n",
    "aspect_ratio_train = np.zeros(num_train)\n",
    "aspect_ratio_val = np.zeros(num_val)\n",
    "aspect_ratio_test = np.zeros(num_test)\n",
    "\n",
    "num_white_regions_train = np.zeros(num_train)\n",
    "num_white_regions_val = np.zeros(num_val)\n",
    "num_white_regions_test = np.zeros(num_test)\n",
    "\n",
    "for i in range(num_train):\n",
    "    aspect_ratio_train[i] = aspect_ratio(train_images_tensor[i].reshape(28, 28).numpy())\n",
    "    num_white_regions_train[i] = num_regions_below_threshold(\n",
    "        train_images_tensor[i].reshape(28, 28)\n",
    "    )\n",
    "\n",
    "for i in range(num_val):\n",
    "    aspect_ratio_val[i] = aspect_ratio(\n",
    "        validation_images_tensor[i].reshape(28, 28).numpy()\n",
    "    )\n",
    "    num_white_regions_val[i] = num_regions_below_threshold(\n",
    "        validation_images_tensor[i].reshape(28, 28)\n",
    "    )\n",
    "\n",
    "for i in range(num_test):\n",
    "    aspect_ratio_test[i] = aspect_ratio(test_images_tensor[i].reshape(28, 28).numpy())\n",
    "    num_white_regions_test[i] = num_regions_below_threshold(\n",
    "        test_images_tensor[i].reshape(28, 28)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape torch.Size([28, 28])\n",
      "Label tensor(0)\n",
      "Aspect ratio 0.7\n",
      "Number of black regions 2.0\n"
     ]
    }
   ],
   "source": [
    "image_id = 0\n",
    "\n",
    "image = train_images_tensor[image_id].reshape(28, 28)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Image shape\", image.shape)\n",
    "\n",
    "print(\"Label\", train_labels_tensor[image_id])\n",
    "print(\"Aspect ratio\", aspect_ratio_train[image_id])\n",
    "print(\"Number of black regions\", num_white_regions_train[image_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute datasets\n",
    "\n",
    "features_train = np.zeros((num_train, len_haar_features + 2))\n",
    "features_val = np.zeros((num_val, len_haar_features + 2))\n",
    "features_test = np.zeros((num_test, len_haar_features + 2))\n",
    "\n",
    "features_train_resized = np.zeros((num_train, len_haar_features_resized + 2))\n",
    "features_val_resized = np.zeros((num_val, len_haar_features_resized + 2))\n",
    "features_test_resized = np.zeros((num_test, len_haar_features_resized + 2))\n",
    "\n",
    "for i in range(num_train):\n",
    "    haar_features = compute_haar_features(\n",
    "        train_images_tensor[i].reshape(28, 28).numpy()\n",
    "    )\n",
    "    features_train[i, :] = np.hstack(\n",
    "        (haar_features, aspect_ratio_train[i], num_white_regions_train[i])\n",
    "    )\n",
    "\n",
    "for i in range(num_val):\n",
    "    haar_features = compute_haar_features(\n",
    "        validation_images_tensor[i].reshape(28, 28).numpy()\n",
    "    )\n",
    "    features_val[i, :] = np.hstack(\n",
    "        (haar_features, aspect_ratio_val[i], num_white_regions_val[i])\n",
    "    )\n",
    "\n",
    "for i in range(num_test):\n",
    "    haar_features = compute_haar_features(test_images_tensor[i].reshape(28, 28).numpy())\n",
    "    features_test[i, :] = np.hstack(\n",
    "        (haar_features, aspect_ratio_test[i], num_white_regions_test[i])\n",
    "    )\n",
    "\n",
    "for i in range(num_train):\n",
    "    haar_features = compute_haar_features(\n",
    "        train_images_tensor_resized[i].reshape(20, 20)\n",
    "    )\n",
    "    features_train_resized[i, :] = np.hstack(\n",
    "        (haar_features, aspect_ratio_train[i], num_white_regions_train[i])\n",
    "    )\n",
    "\n",
    "for i in range(num_val):\n",
    "    haar_features = compute_haar_features(\n",
    "        validation_images_tensor_resized[i].reshape(20, 20)\n",
    "    )\n",
    "    features_val_resized[i, :] = np.hstack(\n",
    "        (haar_features, aspect_ratio_val[i], num_white_regions_val[i])\n",
    "    )\n",
    "\n",
    "for i in range(num_test):\n",
    "    haar_features = compute_haar_features(test_images_tensor_resized[i].reshape(20, 20))\n",
    "    features_test_resized[i, :] = np.hstack(\n",
    "        (haar_features, aspect_ratio_test[i], num_white_regions_test[i])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_normalized = torch.tensor(scaler.fit_transform(features_train))\n",
    "val_features_normalized = torch.tensor(scaler.transform(features_val))\n",
    "test_features_normalized = torch.tensor(scaler.transform(features_test))\n",
    "\n",
    "train_features_resized_normalized = torch.tensor(\n",
    "    scaler.fit_transform(features_train_resized)\n",
    ")\n",
    "val_features_resized_normalized = torch.tensor(scaler.transform(features_val_resized))\n",
    "test_features_resized_normalized = torch.tensor(scaler.transform(features_test_resized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_normalized = train_features_normalized.float()\n",
    "val_features_normalized = val_features_normalized.float()\n",
    "test_features_normalized = test_features_normalized.float()\n",
    "\n",
    "train_features_resized_normalized = train_features_resized_normalized.float()\n",
    "val_features_resized_normalized = val_features_resized_normalized.float()\n",
    "test_features_resized_normalized = test_features_resized_normalized.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out a neural network, on the new 28x28 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100], Loss: 0.4394, validation loss: 0.3833\n",
      "Epoch [200], Loss: 0.2931, validation loss: 0.2341\n",
      "Epoch [300], Loss: 0.2426, validation loss: 0.1850\n",
      "Epoch [400], Loss: 0.2139, validation loss: 0.1595\n",
      "Epoch [500], Loss: 0.1948, validation loss: 0.1425\n",
      "Epoch [600], Loss: 0.1805, validation loss: 0.1301\n",
      "Epoch [700], Loss: 0.1693, validation loss: 0.1207\n",
      "Epoch [800], Loss: 0.1602, validation loss: 0.1133\n",
      "Epoch [900], Loss: 0.1527, validation loss: 0.1074\n",
      "Epoch [1000], Loss: 0.1465, validation loss: 0.1029\n",
      "Epoch [1100], Loss: 0.1413, validation loss: 0.0997\n",
      "Epoch [1200], Loss: 0.1368, validation loss: 0.0970\n",
      "Epoch [1300], Loss: 0.1329, validation loss: 0.0945\n",
      "Epoch [1400], Loss: 0.1295, validation loss: 0.0921\n",
      "Epoch [1500], Loss: 0.1265, validation loss: 0.0902\n",
      "Epoch [1600], Loss: 0.1238, validation loss: 0.0884\n",
      "Epoch [1700], Loss: 0.1214, validation loss: 0.0870\n",
      "Epoch [1800], Loss: 0.1193, validation loss: 0.0859\n",
      "Epoch [1900], Loss: 0.1174, validation loss: 0.0849\n",
      "Epoch [2000], Loss: 0.1157, validation loss: 0.0838\n",
      "Epoch [2100], Loss: 0.1141, validation loss: 0.0831\n",
      "Epoch [2200], Loss: 0.1126, validation loss: 0.0823\n",
      "Epoch [2300], Loss: 0.1113, validation loss: 0.0817\n",
      "Epoch [2400], Loss: 0.1100, validation loss: 0.0809\n",
      "Epoch [2500], Loss: 0.1088, validation loss: 0.0802\n",
      "Epoch [2600], Loss: 0.1078, validation loss: 0.0796\n",
      "Early stopping\n",
      "Accuracy: 0.9785\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9785"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_dim = train_features_normalized.shape[1]\n",
    "output_dim = len(set(train_labels))  # Assuming train_labels are class indices\n",
    "hidden_dim = int((input_dim + output_dim) / 2)\n",
    "\n",
    "# Instantiate the model\n",
    "model_medium2 = SimpleNN(\n",
    "    input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_medium2.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop with L1 regularization\n",
    "lambda_l1 = 0.0001  # L1 regularization coefficient\n",
    "\n",
    "validation_losses = []\n",
    "epoch = 0\n",
    "\n",
    "model_states = []\n",
    "\n",
    "while True:\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model_medium2(train_features_normalized)\n",
    "\n",
    "    loss = criterion(outputs, train_labels_tensor)\n",
    "\n",
    "    # Add L1 regularization\n",
    "    l1_reg = torch.tensor(0.0, requires_grad=True)\n",
    "    for param in model_medium2.parameters():\n",
    "        l1_reg = l1_reg + torch.norm(param, 1)\n",
    "    loss += lambda_l1 * l1_reg\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print progress\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(\n",
    "            f\"Epoch [{epoch + 1}], Loss: {loss.item():.4f}, validation loss: {validation_losses[-1]:.4f}\"\n",
    "        )\n",
    "\n",
    "    # store model state\n",
    "    model_states.append(model_medium2.state_dict())\n",
    "\n",
    "    # Compute validation loss\n",
    "    with torch.no_grad():\n",
    "        outputs = model_medium2(val_features_normalized)\n",
    "        loss = criterion(outputs, validation_labels_tensor)\n",
    "        validation_losses.append(loss.item())\n",
    "\n",
    "    # Check for early stopping if no improvement in validation loss in last 10 epochs\n",
    "    if epoch > 10 and validation_losses[-1] > validation_losses[-10]:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "    epoch += 1\n",
    "\n",
    "best_model_state = model_states[np.argmin(validation_losses)]\n",
    "model_medium2.load_state_dict(best_model_state)\n",
    "\n",
    "evaluate_model(model_medium2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of weight parameters: 9800\n",
      "Number of changed weight parameters: 7953\n",
      "Number of bias parameters: 80\n",
      "Number of changed bias parameters: 49\n",
      "Percentage of weights pruned: 81.15%\n",
      "Percentage of biases pruned: 61.25%\n",
      "Remaining number of non-zero weights: 1847\n",
      "Remaining number of non-zero biases: 31\n",
      "Accuracy: 0.9745\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9745"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_medium2_pruned = copy.deepcopy(model_medium2)\n",
    "model_medium2_pruned = prune_pytorch_network(model_medium2_pruned, 1e-1, 1e-1)\n",
    "\n",
    "evaluate_model(model_medium2_pruned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out a neural network, on the new 20x20 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100], Loss: 1.0097, validation loss: 0.9938\n",
      "Epoch [200], Loss: 0.4981, validation loss: 0.4804\n",
      "Epoch [300], Loss: 0.3547, validation loss: 0.3365\n",
      "Epoch [400], Loss: 0.2938, validation loss: 0.2756\n",
      "Epoch [500], Loss: 0.2603, validation loss: 0.2410\n",
      "Epoch [600], Loss: 0.2391, validation loss: 0.2182\n",
      "Epoch [700], Loss: 0.2240, validation loss: 0.2020\n",
      "Epoch [800], Loss: 0.2127, validation loss: 0.1894\n",
      "Epoch [900], Loss: 0.2037, validation loss: 0.1793\n",
      "Epoch [1000], Loss: 0.1962, validation loss: 0.1708\n",
      "Epoch [1100], Loss: 0.1898, validation loss: 0.1638\n",
      "Epoch [1200], Loss: 0.1842, validation loss: 0.1578\n",
      "Epoch [1300], Loss: 0.1792, validation loss: 0.1525\n",
      "Epoch [1400], Loss: 0.1748, validation loss: 0.1481\n",
      "Epoch [1500], Loss: 0.1708, validation loss: 0.1442\n",
      "Epoch [1600], Loss: 0.1671, validation loss: 0.1407\n",
      "Epoch [1700], Loss: 0.1637, validation loss: 0.1378\n",
      "Epoch [1800], Loss: 0.1606, validation loss: 0.1350\n",
      "Epoch [1900], Loss: 0.1576, validation loss: 0.1323\n",
      "Epoch [2000], Loss: 0.1548, validation loss: 0.1298\n",
      "Epoch [2100], Loss: 0.1523, validation loss: 0.1275\n",
      "Epoch [2200], Loss: 0.1499, validation loss: 0.1254\n",
      "Epoch [2300], Loss: 0.1477, validation loss: 0.1235\n",
      "Epoch [2400], Loss: 0.1455, validation loss: 0.1216\n",
      "Epoch [2500], Loss: 0.1435, validation loss: 0.1198\n",
      "Epoch [2600], Loss: 0.1416, validation loss: 0.1182\n",
      "Epoch [2700], Loss: 0.1399, validation loss: 0.1168\n",
      "Epoch [2800], Loss: 0.1384, validation loss: 0.1155\n",
      "Epoch [2900], Loss: 0.1370, validation loss: 0.1144\n",
      "Epoch [3000], Loss: 0.1356, validation loss: 0.1134\n",
      "Epoch [3100], Loss: 0.1344, validation loss: 0.1124\n",
      "Epoch [3200], Loss: 0.1332, validation loss: 0.1115\n",
      "Epoch [3300], Loss: 0.1321, validation loss: 0.1108\n",
      "Epoch [3400], Loss: 0.1311, validation loss: 0.1101\n",
      "Epoch [3500], Loss: 0.1301, validation loss: 0.1094\n",
      "Epoch [3600], Loss: 0.1292, validation loss: 0.1088\n",
      "Epoch [3700], Loss: 0.1283, validation loss: 0.1083\n",
      "Epoch [3800], Loss: 0.1274, validation loss: 0.1077\n",
      "Epoch [3900], Loss: 0.1267, validation loss: 0.1071\n",
      "Epoch [4000], Loss: 0.1260, validation loss: 0.1067\n",
      "Early stopping\n",
      "Accuracy: 0.9714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9714"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_dim = train_features_resized_normalized.shape[1]\n",
    "output_dim = len(set(train_labels))  # Assuming train_labels are class indices\n",
    "hidden_dim = int((input_dim + output_dim) / 2)\n",
    "\n",
    "# Instantiate the model\n",
    "model_medium2_resized = SimpleNN(\n",
    "    input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_medium2_resized.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop with L1 regularization\n",
    "lambda_l1 = 0.0001  # L1 regularization coefficient\n",
    "\n",
    "validation_losses = []\n",
    "epoch = 0\n",
    "\n",
    "model_states = []\n",
    "\n",
    "while True:\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model_medium2_resized(train_features_resized_normalized)\n",
    "\n",
    "    loss = criterion(outputs, train_labels_tensor)\n",
    "\n",
    "    # Add L1 regularization\n",
    "    l1_reg = torch.tensor(0.0, requires_grad=True)\n",
    "    for param in model_medium2_resized.parameters():\n",
    "        l1_reg = l1_reg + torch.norm(param, 1)\n",
    "    loss += lambda_l1 * l1_reg\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print progress\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(\n",
    "            f\"Epoch [{epoch + 1}], Loss: {loss.item():.4f}, validation loss: {validation_losses[-1]:.4f}\"\n",
    "        )\n",
    "\n",
    "    # store model state\n",
    "    model_states.append(model_medium2_resized.state_dict())\n",
    "\n",
    "    # Compute validation loss\n",
    "    with torch.no_grad():\n",
    "        outputs = model_medium2_resized(val_features_resized_normalized)\n",
    "        loss = criterion(outputs, validation_labels_tensor)\n",
    "        validation_losses.append(loss.item())\n",
    "\n",
    "    # Check for early stopping if no improvement in validation loss in last 10 epochs\n",
    "    if epoch > 10 and validation_losses[-1] > validation_losses[-10]:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "    epoch += 1\n",
    "\n",
    "best_model_state = model_states[np.argmin(validation_losses)]\n",
    "model_medium2_resized.load_state_dict(best_model_state)\n",
    "\n",
    "evaluate_model(model_medium2_resized, resized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of weight parameters: 1922\n",
      "Number of changed weight parameters: 894\n",
      "Number of bias parameters: 41\n",
      "Number of changed bias parameters: 10\n",
      "Percentage of weights pruned: 46.51%\n",
      "Percentage of biases pruned: 24.39%\n",
      "Remaining number of non-zero weights: 1028\n",
      "Remaining number of non-zero biases: 31\n",
      "Accuracy: 0.9702\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9702"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_medium2_resized_pruned = copy.deepcopy(model_medium2_resized)\n",
    "model_medium2_resized_pruned = prune_pytorch_network(\n",
    "    model_medium2_resized_pruned, 1e-1, 1e-1\n",
    ")\n",
    "\n",
    "evaluate_model(model_medium2_resized_pruned, resized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight \t torch.Size([31, 52])\n",
      "fc1.bias \t torch.Size([31])\n",
      "fc2.weight \t torch.Size([10, 31])\n",
      "fc2.bias \t torch.Size([10])\n",
      "fc1.weight \t tensor([[-0.1457,  0.0000,  0.0000,  ..., -0.1208,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ..., -0.1804,  0.0000,  0.0000],\n",
      "        [ 0.0000, -0.1672,  0.5704,  ...,  0.0000,  0.0000,  0.1628],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.8005, -0.9213],\n",
      "        [ 0.4910,  0.0000,  0.0000,  ...,  0.0000,  0.1205,  0.0000],\n",
      "        [ 0.0000,  0.7164, -0.6793,  ...,  0.0000,  0.0000, -0.2302]])\n",
      "fc1.bias \t tensor([ 0.3703,  0.1497,  0.9252,  0.7694,  0.2327, -0.1245,  0.3859,  0.1467,\n",
      "         0.5328,  0.5766,  0.0000,  0.1754,  0.1570,  0.3784,  0.0000,  0.0000,\n",
      "         0.4044,  0.2387,  0.1343,  0.0000,  0.5028,  0.6376,  0.5517,  0.1667,\n",
      "         0.6389,  0.4007,  0.2228,  0.3682,  0.5501,  0.1588, -0.1081])\n",
      "fc2.weight \t tensor([[ 0.0000, -0.8436,  0.7421,  0.3145,  0.6317,  0.2764, -0.9505,  0.3824,\n",
      "         -1.0316,  0.0000, -0.6444,  0.2943, -0.6174,  0.4459, -0.6827,  0.3528,\n",
      "          0.6066, -0.7019, -0.1604, -0.5303, -0.8808,  0.2669,  0.4567, -0.4648,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000, -0.7364, -0.2286,  0.6637],\n",
      "        [-0.3628, -1.0329, -0.4100, -0.4774,  0.1389,  0.0000, -0.4175, -0.6277,\n",
      "         -0.1930,  0.8022, -0.5196,  0.4716,  0.0000, -0.8477, -0.1067,  0.0000,\n",
      "         -0.9019,  0.4444, -0.5627,  0.6974, -0.2948,  0.1045,  0.3102,  0.2478,\n",
      "          0.7170,  1.2600, -0.7098, -0.3746,  1.1563, -0.5991,  0.0000],\n",
      "        [-0.8695,  0.5932, -0.1400,  0.0000, -0.3588,  0.1840, -0.3957, -0.9202,\n",
      "         -0.3174, -0.7727,  0.7170,  0.6617,  0.8326,  0.6312, -0.2590, -0.3394,\n",
      "          0.7626,  0.0000,  1.0193,  0.0000, -0.3562,  0.4361,  0.4099,  0.5038,\n",
      "          0.4704,  0.0000, -0.3824, -0.2034, -0.3836, -1.3858, -0.5232],\n",
      "        [ 0.9601,  0.7866,  0.0000, -0.4931, -1.0539, -0.7878,  0.4281,  0.7172,\n",
      "          0.3040,  0.0000, -0.3005, -0.4074,  0.1954,  0.0000,  0.0000, -0.3346,\n",
      "          1.0174, -1.0311,  0.4944,  0.5326, -0.5748,  0.7134, -1.1122,  0.9642,\n",
      "          0.5039,  0.0000,  0.0000, -0.7243, -0.2482,  0.1120, -0.9603],\n",
      "        [-0.4492, -0.9544, -1.4454,  0.0000, -0.5753,  0.6885,  0.2771,  0.0000,\n",
      "          0.2198,  0.6528,  0.6542, -0.6155,  1.6867, -0.1416,  0.8309,  0.7444,\n",
      "         -0.2813,  0.1141, -0.6212,  0.2733,  0.4773, -0.9097, -0.2395, -0.9552,\n",
      "          0.0000, -0.2359, -0.8688,  1.1078, -0.1564, -0.4903, -0.1173],\n",
      "        [ 0.3563,  0.6737, -0.2409, -0.1744,  0.1481, -0.6930,  0.6589,  0.3621,\n",
      "          0.7692,  0.0000, -0.3548, -0.9881,  0.0000,  0.0000,  0.0000, -1.4564,\n",
      "          0.0000,  0.4144,  0.0000, -1.0867,  0.6022, -0.9108,  0.3906,  0.0000,\n",
      "         -1.4314,  0.0000,  1.0618, -0.8915,  0.2281,  0.1188,  0.8971],\n",
      "        [-0.3468,  0.2345, -0.5260,  1.8111,  0.4043, -1.3183, -0.9109,  0.0000,\n",
      "         -0.2523,  0.2332, -0.4330,  0.1729,  0.4093, -0.3544, -0.6351,  0.4571,\n",
      "         -0.1859, -0.5771, -0.9522,  0.0000, -0.4841, -0.7517,  0.0000, -0.6298,\n",
      "          0.0000,  0.0000,  0.7561,  0.0000, -0.1175, -0.7028,  0.7516],\n",
      "        [-0.8780, -0.4229,  0.0000, -0.6577, -0.4697,  0.5960, -1.1681, -0.3868,\n",
      "          0.1491, -0.5527,  0.2820,  0.3591, -0.4836,  0.3287,  0.5358,  0.2272,\n",
      "          0.0000,  0.3369,  0.4400,  1.1490, -0.7031, -0.4211,  0.1313,  1.0099,\n",
      "          0.1340, -0.7723,  0.0000,  0.6544,  0.0000,  0.9599, -0.1786],\n",
      "        [ 0.6028,  0.5414,  0.5386,  0.0000,  0.7212, -0.5090,  1.0851,  0.0000,\n",
      "         -0.4153, -0.6271, -0.8573,  0.9084, -1.2111, -1.2127, -1.3860, -0.3897,\n",
      "         -0.5288,  0.0000,  0.4173, -0.8588,  0.7053,  0.7188,  0.0000, -0.4575,\n",
      "          0.7065, -0.1454,  0.2806,  0.3087, -0.8773,  0.0000,  0.1857],\n",
      "        [ 0.0000, -0.4694,  0.9185,  0.0000, -0.7022,  0.2799,  0.8776, -0.4555,\n",
      "         -0.7645, -0.6707,  0.5379, -0.7583,  0.0000, -0.6986,  1.0440, -0.1264,\n",
      "         -0.2921,  1.1392, -0.8628,  0.0000,  0.4755,  0.6607, -1.1210, -0.6028,\n",
      "         -0.4271, -0.6314,  0.3237,  0.1661,  0.0000,  0.8351, -0.4911]])\n",
      "fc2.bias \t tensor([ 0.0000, -0.2004,  0.0000,  0.0000,  0.0000,  0.3526,  0.0000, -0.6548,\n",
      "         0.3168,  0.0000])\n"
     ]
    }
   ],
   "source": [
    "# print model_medium2_resized_pruned weights\n",
    "for param_tensor in model_medium2_resized_pruned.state_dict():\n",
    "    print(\n",
    "        param_tensor,\n",
    "        \"\\t\",\n",
    "        model_medium2_resized_pruned.state_dict()[param_tensor].size(),\n",
    "    )\n",
    "\n",
    "# print model_medium2_resized_pruned weights\n",
    "for param_tensor in model_medium2_resized_pruned.state_dict():\n",
    "    print(param_tensor, \"\\t\", model_medium2_resized_pruned.state_dict()[param_tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kp/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp_network = MLPClassifier(\n",
    "    hidden_layer_sizes=(31,), max_iter=1, alpha=0.0001, solver=\"adam\", random_state=0\n",
    ")\n",
    "mlp_network.fit(train_features_resized_normalized, train_labels_tensor)\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(\"sklearn_mlp_initialized.pkl\", \"wb\") as f:\n",
    "    pickle.dump(mlp_network, f)\n",
    "\n",
    "# store the pytorch model model_medium2_resized_pruned\n",
    "torch.save(model_medium2_resized_pruned.state_dict(), \"pytorch_mlp_trained.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9702\n",
      "Number of neurons per layer: [52, 31, 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kp/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def pytorch_to_sklearn(pytorch_model):\n",
    "\n",
    "    # Extract weights and biases from PyTorch model\n",
    "    fc1_weight = pytorch_model.fc1.weight.data\n",
    "    fc1_bias = pytorch_model.fc1.bias.data\n",
    "    fc2_weight = pytorch_model.fc2.weight.data\n",
    "    fc2_bias = pytorch_model.fc2.bias.data\n",
    "\n",
    "    # Get the sizes for initialization\n",
    "    input_size = fc1_weight.shape[1]\n",
    "    hidden_size = fc1_weight.shape[0]\n",
    "    output_size = fc2_weight.shape[0]\n",
    "\n",
    "    # Initialize sklearn MLP\n",
    "    sklearn_mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=(hidden_size,), activation=\"relu\", max_iter=1\n",
    "    )\n",
    "\n",
    "    # To ensure the model doesn't change the weights during the dummy fit, we set warm_start=True\n",
    "    sklearn_mlp.warm_start = True\n",
    "\n",
    "    # Dummy fit to initialize weights (necessary step before setting the weights)\n",
    "    sklearn_mlp.fit(np.zeros((output_size, input_size)), list(range(output_size)))\n",
    "\n",
    "    # Set the weights and biases\n",
    "    sklearn_mlp.coefs_[0] = fc1_weight.t().numpy()\n",
    "    sklearn_mlp.intercepts_[0] = fc1_bias.numpy()\n",
    "    sklearn_mlp.coefs_[1] = fc2_weight.t().numpy()\n",
    "    sklearn_mlp.intercepts_[1] = fc2_bias.numpy()\n",
    "\n",
    "    return sklearn_mlp\n",
    "\n",
    "\n",
    "# Convert the example PyTorch MLP to sklearn MLP\n",
    "example_sklearn_mlp = pytorch_to_sklearn(model_medium2_resized_pruned)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = example_sklearn_mlp.score(\n",
    "    test_features_resized_normalized.numpy(), test_labels\n",
    ")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "layers_sizes = [example_sklearn_mlp.coefs_[0].shape[0]] + [\n",
    "    coef.shape[1] for coef in example_sklearn_mlp.coefs_\n",
    "]\n",
    "\n",
    "print(\"Number of neurons per layer:\", layers_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pytorch_to_sklearn(pytorch_model):\n",
    "    # Extract weights and biases from the PyTorch model\n",
    "    fc1_weight = pytorch_model.fc1.weight.data.numpy()\n",
    "    fc1_bias = pytorch_model.fc1.bias.data.numpy()\n",
    "\n",
    "    fc2_weight = pytorch_model.fc2.weight.data.numpy()\n",
    "    fc2_bias = pytorch_model.fc2.bias.data.numpy()\n",
    "\n",
    "    # Define the structure of the sklearn MLP model\n",
    "    # Here, we assume that the PyTorch model's output_dim is 1, which corresponds to a binary classification in sklearn\n",
    "    sklearn_mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=(fc1_weight.shape[0],),\n",
    "        activation=\"relu\",\n",
    "        max_iter=1,  # Initialize and set weights manually, so no need to iterate\n",
    "        warm_start=True,\n",
    "    )  # This ensures we can update weights after initialization\n",
    "\n",
    "    # Initialize the sklearn model (necessary before setting weights)\n",
    "    sklearn_mlp.fit(np.zeros((1, fc1_weight.shape[1])), [0])\n",
    "\n",
    "    # Set weights and biases in the sklearn model\n",
    "    sklearn_mlp.coefs_[0] = fc1_weight\n",
    "    sklearn_mlp.intercepts_[0] = fc1_bias\n",
    "    sklearn_mlp.coefs_[1] = fc2_weight\n",
    "    sklearn_mlp.intercepts_[1] = fc2_bias\n",
    "\n",
    "    return sklearn_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kp/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sklearn_model = convert_pytorch_to_sklearn(model_medium2_resized_pruned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input layer: 52 neurons\n",
      "Hidden layer 1: 31 neurons\n",
      "Output layer: 10 neurons\n"
     ]
    }
   ],
   "source": [
    "def print_sklearn_mlp_architecture(mlp):\n",
    "    # Get the input layer size\n",
    "    input_size = mlp.coefs_[0].shape[1]\n",
    "    print(f\"Input layer: {input_size} neurons\")\n",
    "\n",
    "    # Loop through the hidden layers\n",
    "    for idx, coef_matrix in enumerate(mlp.coefs_[:-1]):\n",
    "        print(f\"Hidden layer {idx + 1}: {coef_matrix.shape[0]} neurons\")\n",
    "\n",
    "    # Output layer\n",
    "    print(f\"Output layer: {mlp.coefs_[-1].shape[0]} neurons\")\n",
    "\n",
    "\n",
    "print_sklearn_mlp_architecture(sklearn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 31 is different from 52)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/kp/dev/python-sdk/zkml-research/MNIST_preprocessing/6_presentation.ipynb Cell 50\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kp/dev/python-sdk/zkml-research/MNIST_preprocessing/6_presentation.ipynb#Y103sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m accuracy \u001b[39m=\u001b[39m sklearn_model\u001b[39m.\u001b[39;49mscore(test_features_resized_normalized\u001b[39m.\u001b[39;49mnumpy(), test_labels)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/base.py:705\u001b[0m, in \u001b[0;36mClassifierMixin.score\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[39mReturn the mean accuracy on the given test data and labels.\u001b[39;00m\n\u001b[1;32m    682\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[39m    Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\u001b[39;00m\n\u001b[1;32m    702\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    703\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m accuracy_score\n\u001b[0;32m--> 705\u001b[0m \u001b[39mreturn\u001b[39;00m accuracy_score(y, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(X), sample_weight\u001b[39m=\u001b[39msample_weight)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1160\u001b[0m, in \u001b[0;36mMLPClassifier.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Predict using the multi-layer perceptron classifier.\u001b[39;00m\n\u001b[1;32m   1148\u001b[0m \n\u001b[1;32m   1149\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[39m    The predicted classes.\u001b[39;00m\n\u001b[1;32m   1158\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1159\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m-> 1160\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict(X)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1164\u001b[0m, in \u001b[0;36mMLPClassifier._predict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_predict\u001b[39m(\u001b[39mself\u001b[39m, X, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m   1163\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Private predict method with optional input validation\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1164\u001b[0m     y_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_pass_fast(X, check_input\u001b[39m=\u001b[39;49mcheck_input)\n\u001b[1;32m   1166\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1167\u001b[0m         y_pred \u001b[39m=\u001b[39m y_pred\u001b[39m.\u001b[39mravel()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:215\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._forward_pass_fast\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    213\u001b[0m hidden_activation \u001b[39m=\u001b[39m ACTIVATIONS[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation]\n\u001b[1;32m    214\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_layers_ \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m--> 215\u001b[0m     activation \u001b[39m=\u001b[39m safe_sparse_dot(activation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcoefs_[i])\n\u001b[1;32m    216\u001b[0m     activation \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintercepts_[i]\n\u001b[1;32m    217\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_layers_ \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/utils/extmath.py:193\u001b[0m, in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    191\u001b[0m         ret \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(a, b)\n\u001b[1;32m    192\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 193\u001b[0m     ret \u001b[39m=\u001b[39m a \u001b[39m@\u001b[39;49m b\n\u001b[1;32m    195\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    196\u001b[0m     sparse\u001b[39m.\u001b[39missparse(a)\n\u001b[1;32m    197\u001b[0m     \u001b[39mand\u001b[39;00m sparse\u001b[39m.\u001b[39missparse(b)\n\u001b[1;32m    198\u001b[0m     \u001b[39mand\u001b[39;00m dense_output\n\u001b[1;32m    199\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(ret, \u001b[39m\"\u001b[39m\u001b[39mtoarray\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    200\u001b[0m ):\n\u001b[1;32m    201\u001b[0m     \u001b[39mreturn\u001b[39;00m ret\u001b[39m.\u001b[39mtoarray()\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 31 is different from 52)"
     ]
    }
   ],
   "source": [
    "accuracy = sklearn_model.score(test_features_resized_normalized.numpy(), test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
